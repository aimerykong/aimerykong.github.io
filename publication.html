
<!DOCTYPE html>

<html lang="en">
<head>
	<meta charset="utf-8" />
	<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Open+Sans" />
	<link rel="stylesheet" type="text/css" href='static/css/bootstrap.min.css' />
	<link rel="stylesheet" type="text/css" href='static/css/styles.css' />
	<script src=''></script>
	<title>Shu Kong</title>
	<link rel="icon" href="aimerykong_files/profile2.png" type="img/jpg">
</head>

<body>

	<div class="my-container">
		<h1> Shu Kong </h1>
		<span> Assistant Professor; University of Macau, Texas A&M University <a href="https://github.com/aimerykong">[GitHub]</a>  <a href="http://scholar.google.com/citations?user=sm9FdLoAAAAJ&hl=en">[Google Scholar]</a> </span>

		<div class="btn-group btn-group-justified" role="group" aria-label="Justified button group">
			<a href="index.html" class="btn btn-default" role="button">Home</a>
			<a href="publication.html" class="btn btn-default" role="button">Publication</a>
			<a href="group.html" class="btn btn-default" role="button">Group</a>
			<a href="contact.html" class="btn btn-default" role="button">Contact</a>
    	</div>



<div class="content">
<div class="row">


<h3> Papers</h3>





<font face="helvetica, ariel, 'sans serif'">
<table cellspacing="15"> 
<tbody>




<tr>
<td width="35%" height=150>
<a href="https://arxiv.org/abs/2404.16972">
<img src="aimerykong_files/crisp.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/abs/2404.16972"><b>CriSp: Leveraging Tread Depth Maps for Enhanced Crime-Scene Shoeprint Matching</b></a>
<br>
S. Shafique, S. Kong, C. Fowlkes
<br>
In ECCV, 2024<br>
[<a href="https://github.com/Samia067/CriSp">webpage</a>]
[<a href="https://arxiv.org/abs/2404.16972">paper</a>]
[<a href="https://github.com/Samia067/CriSp?tab=readme-ov-file#datasets">dataset</a>]
[<a href="https://github.com/Samia067/CriSp">code</a>]
<br>
</td>



<tr>
<td width="35%" height=150>
<a href="https://github.com/WangYZ1608/Knowledge-Distillation-via-ND">
<img src="aimerykong_files/KD-dino.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/abs/2305.17007"><b>Improving Knowledge Distillation via Regularizing Feature Norm and Direction
</b></a>
<br>
Y. Wang, L. Cheng, M. Duan, Y. Wang, Z. Feng, S. Kong
<br>
In ECCV, 2024
<br>
[<a href="https://arxiv.org/abs/2305.17007">paper</a>]
[<a href="https://github.com/WangYZ1608/Knowledge-Distillation-via-ND">github</a>]
<br>
</td>


	
<tr>
<td width="35%" height=150>
<a href="https://arxiv.org/abs/2406.11148">
<img src="aimerykong_files/SWAT.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="./"><b>Few-Shot Recognition via Stage-Wise Augmented Finetuning</b></a>
<br>
T. Liu, H. Zhang, S. Parashar, S. Kong
<br>
<a href="https://arxiv.org/abs/2406.11148">arXiv:2406.11148</a>, 2024<br>
[<a href="https://tian1327.github.io/SWAT/">website</a>] 
[<a href="https://arxiv.org/abs/2406.11148">paer</a>] 
[<a href="https://github.com/tian1327/SWAT">code</a>]
<br>
</td>	


	
<tr>
<td width="35%" height=150>
<a href="./">
<img src="aimerykong_files/IJCV2024.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://github.com/g-meghana-reddy/open-world-panoptic-segmentation"><b>Lidar Panoptic Segmentation in an Open World</b></a>
<br>
A. S. Chakravarthy, M. R. Ganesina, P. Hu, L. Leal-Taixé, S. Kong, D. Ramanan, A. Osep
<br>
<a href="https://link.springer.com/journal/11263">International Journal of Computer Vision (IJCV)</a>, 2024<br>
[<a href="./">paper to appear</a>] 
[<a href="https://github.com/g-meghana-reddy/open-world-panoptic-segmentation">code</a>]
<br>
</td>	



	





<tr>
<td width="35%" height=150>
<a href="https://openreview.net/forum?id=HPXRzM9BYZ">
<img src="aimerykong_files/icml24-splahy-fig.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="./"><b>LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies</b></a>
<br>
J. Shi, G. Gare, J. Tian, S. Chai, Z. Lin, A. Vasudevan, D. Feng, F. Ferroni, S. Kong
<br>
In <a href="https://icml.cc/Conferences/2024">ICML</a>, 2024 (<font color=ff3399>oral presentation</font>)<br>
[<a href="https://elvishelvis.github.io/papers/lca/">webpage</a>]
[<a href="https://openreview.net/forum?id=HPXRzM9BYZ">paper</a>] 
[<a href="https://github.com/ElvishElvis/LCA-on-the-line">code</a>]
[<a href="https://elvishelvis.github.io/papers/lca/ICML24_LCA_poster.pdf">poster</a>]
<br>
</td>	




	

	


<tr>
<td width="35%" height=150>
<a href="https://arxiv.org/abs/2407.06114v1">
<img src="aimerykong_files/UUO-mocap.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/abs/2407.06114v1"><b>Towards Unstructured Unlabeled Optical Mocap: A Video Helps!</b></a>
<br>
N. Milef, J. Keyser, S. Kong
<br>
In <a href="https://s2024.siggraph.org/">SIGGRAPH</a>, 2024<br>
[<a href="http://www.nicholasmilef.com/projects/towards_unstructured_unlabeled_optical_mocap">website</a>]
[<a href="https://www.youtube.com/watch?v=FVAwfc5vXV0">video demo</a>] 
[<a href="https://github.com/NicholasMilef/UUO-Mocap">code</a>]
[<a href="https://arxiv.org/abs/2407.06114v1">paper</a>] 
[<a href="https://drive.google.com/drive/folders/1S-pHcope9VYS-h9ye2LlV2jpfYYEXELI?usp=drive_link">data</a>]
<br>
</td>	

	


<tr>
<td width="35%" height=150>
<a href="https://arxiv.org/abs/2401.12425">
<img src="aimerykong_files/VLM-LT.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/abs/2401.12425"><b>The Neglected Tails of Vision-Language Models</b></a>
<br>
S. Parashar, Z. Lin, T. Liu, X. Dong, Y. Li, D. Ramanan, J. Caverlee, S. Kong
<br>
In <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR</a>, 2024<br>
[<a href="https://shubhamprshr27.github.io/neglected-tails-of-vlms/">webpage</a>]
[<a href="https://arxiv.org/abs/2401.12425">paper</a>]
[<a href="https://github.com/shubhamprshr27/NeglectedTailsVLM">code</a>]
<br>
</td>


<tr>
<td width="35%" height=150>
<a href="https://arxiv.org/abs/2312.04117">
<img src="aimerykong_files/IT3DEgo.png" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/abs/2312.04117"><b>Instance Tracking in 3D Scenes from Egocentric Videos
</b></a>
<br>
Y. Zhao, H. Ma, S. Kong, C. Fowlkes
<br>
In <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR</a>, 2024<br>
[<a href="https://arxiv.org/abs/2312.04117">paper</a>]
[<a href="https://github.com/IT3DEgo/IT3DEgo/">code</a>]
[<a href="https://drive.google.com/file/d/1VVszWG4mmm0g3ai3EoZw-3cGNBmZCN-9/view?usp=sharing">data</a>]
<br>
</td>
		

	

<tr>
<td width="35%" height=150>
<a href="https://arxiv.org/abs/2312.03818">
<img src="aimerykong_files/alpha-CLIP.png" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/abs/2312.03818"><b>Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</b></a>
<br>
Z. Sun, Y. Fang, T. Wu, P. Zhang, Y. Zang, S. Kong, Y. Xiong, D. Lin, J. Wang
<br>
In <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR</a>, 2024<br>
[<a href="https://arxiv.org/abs/2312.03818">paper</a>] 
[<a href="https://huggingface.co/papers/2312.03818">code</a>]
[<a href="https://aleafy.github.io/alpha-clip/">webpage</a>]
<br>
</td>



<tr>
<td width="35%" height=150>
<a href="https://arxiv.org/pdf/2403.06793.pdf">
<img src="aimerykong_files/imageRestoration-deep-prior.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/pdf/2403.06793.pdf"><b>Boosting Image Restoration via Priors from Pre-trained Models
</b></a>
<br>
X. Xu, S. Kong, T. Hu, Z. Liu, H. Bao
<br>
In <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR</a>, 2024<br>
[<a href="https://arxiv.org/pdf/2403.06793.pdf">paper</a>]
<br>
</td>
		

	

<tr>
<td width="35%" height=150>
<a href="https://arxiv.org/abs/2401.15996">
<img src="aimerykong_files/AccessLens.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/abs/2401.15996"><b>AccessLens: Auto-detecting Inaccessibility of Everyday Objects</b></a>
<br>
N. Kwon, Q. Lu, M. H. Qazi, J. Liu, C. Oh, S. Kong, J. Kim
<br>
In <a href="https://chi2024.acm.org/">CHI</a>, 2024<br>
[<a href="https://arxiv.org/abs/2401.15996">paper</a>]
[<a href="https://www.youtube.com/watch?v=HX0KW0VsDjw">video</a>]
[<a href="https://access-lens.web.app/">website</a>]
<br>
</td>
		

<tr>
<td width="35%" height=150>
<a href="https://academic.oup.com/pnasnexus/advance-article/doi/10.1093/pnasnexus/pgad419/7471798">
<img src="aimerykong_files/phylogentic-placement.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://academic.oup.com/pnasnexus/advance-article/doi/10.1093/pnasnexus/pgad419/7471798"><b>Deep Learning Approaches to the Phylogenetic Placement of Extinct Pollen Morphotypes
</b></a>
<br>
<a href="https://scholar.google.com/citations?user=qYMFo1wAAAAJ&hl=en">MÉ Adaime</a>,  
S. Kong, 
<a href="http://publish.illinois.edu/punyasena/">S. Punyasena</a>
<br>
Proceedings of the National Academy of Sciences (PNAS) Nexus, 2024<br>
[<a href="https://academic.oup.com/pnasnexus/advance-article/doi/10.1093/pnasnexus/pgad419/7471798">paper</a>]
[<a href="https://www.igb.illinois.edu/article/machine-learning-used-classify-fossils-extinct-pollen">News</a>]
<br>
</td>

	
	
<tr>
<td width="35%" height=150>
<a href="https://arxiv.org/abs/2404.01064">
<img src="aimerykong_files/roadside3D.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/abs/2404.01064"><b>Roadside Monocular 3D Detection via 2D Detection Prompting</b></a>
<br>
Y. Ma, S. Wei, C. Zhang, W. Hua, Y. Li, S. Kong
<br>
arXiv:2404.01064, 2024<br>
[<a href="https://arxiv.org/abs/2404.01064">paper</a>]
<br>
</td>



	
	
<tr>
<td width="35%" height=150>
<a href="https://arxiv.org/abs/2312.10986">
<img src="aimerykong_files/LT3D-lateFusion.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/abs/2312.10986"><b>Long-Tailed 3D Detection via 2D Late Fusion
</b></a>
<br>
Y. Ma, N. Peri, S. Wei, W. Hua, D. Ramanan, Y. Li, S. Kong
<br>
arXiv:2312.10986, 2024<br>
[<a href="https://arxiv.org/abs/2312.10986">paper</a>]
<br>
</td>


<tr>
<td width="35%" height=150>
<a href="https://arxiv.org/abs/2312.14494">
<img src="aimerykong_files/revisitFSOD.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/abs/2312.14494"><b>Revisiting Few-Shot Object Detection with Vision-Language Models
</b></a>
<br>
A. Madan, N. Peri, S. Kong, D. Ramanan
<br>
arXiv:2312.14494, 2024<br>
[<a href="https://arxiv.org/abs/2312.14494">paper</a>]
<br>
</td>
		




	


<tr>
<td width="35%" height=150>
<a href="http://arxiv.org/abs/2310.09929">
<img src="aimerykong_files/prompt-s-name.png" height="120"  border="0">
</a><br>
</td>
<td>
<a href="http://arxiv.org/abs/2310.09929"><b>Prompting Scientific Names for Zero-Shot Species Recognition</b></a>
<br>
<a href="https://www.linkedin.com/in/shubhamprshr/">S. Parashar</a>,
<a href="http://arxiv.org/abs/2310.09929">Z. Lin</a>,
<a href="http://arxiv.org/abs/2310.09929">Y. Li</a>,
S. Kong
<br>
In EMNLP, 2023
<br>
[<a href="http://arxiv.org/abs/2310.09929">paper</a>]
<br>
</td>




<tr>
<td width="35%" height=150>
<a href="https://github.com/insdet/instance-detection">
<img src="aimerykong_files/insdet.png" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/abs/2310.19257"><b>A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture</b></a>
<br>
<a href="aimerykong_files/insdet.png">Q. Shen</a>,
<a href="https://ics.uci.edu/~yunhaz5/">Y. Zhao</a>,
<a href="https://nahyunkwon.github.io/">N. Kwon</a>,
<a href="https://hcied.info/jeeeunkim.html">J. Kim</a>,
<a href="aimerykong_files/insdet.png">Y. Li</a>,
S. Kong
<br>
In NeurIPS Datasets and Benchmarks, 2023
<br>
[<a href="https://github.com/insdet/instance-detection">github</a>]
[<a href="https://neurips.cc/virtual/2023/poster/73409">paper</a>]
[<a href="https://drive.google.com/drive/folders/1rIRTtqKJGCTifcqJFSVvFshRb-sB0OzP?usp=sharing">dataset</a>]
<br>
</td>


<tr>
<td width="35%" height=150>
<a href="https://github.com/OpenRobotLab/OV_PARTS">
<img src="aimerykong_files/ov-parts.png" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://neurips.cc/virtual/2023/poster/73650"><b>OV-PARTS: Towards Open-Vocabulary Part Segmentation</b></a>
<br>
<a href="aimerykong_files/ov-parts.png">M. Wei</a>,
<a href="aimerykong_files/ov-parts.png">X. Yue</a>,
<a href="aimerykong_files/ov-parts.png">W. Zhang </a>,
S. Kong, 
<a href="https://xh-liu.github.io/">X. Liu</a>,
<a href="https://oceanpang.github.io/">J. Pang</a>
<br>
In NeurIPS Datasets and Benchmarks, 2023
<br>
[<a href="https://github.com/OpenRobotLab/OV_PARTS">github</a>]
[<a href="https://neurips.cc/virtual/2023/poster/73650">paper</a>]
<br>
</td>






	
<tr>
<td width="35%" height=150>
<a href="https://github.com/Samia067/ShoeRinsics">
<img src="aimerykong_files/WACV23-ShoeRinsics.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/abs/2205.02361"><b>Creating a Forensic Database of Shoeprints from Online Shoe Tread Photos</b></a>
<br>
<a href="https://sites.google.com/site/samiashafique067">S. Shafique</a>,
<a href="https://baileykong.com">B. Kong</a>, 
S. Kong<sup>*</sup>, 
<a href="https://www.ics.uci.edu/~fowlkes">C. Fowlkes</a><sup>*</sup>
<br>
In WACV 2023
<br>
[<a href="https://github.com/Samia067/ShoeRinsics">github</a>]
[<a href="https://arxiv.org/abs/2205.02361">paper</a>]
<br>
</td>




<tr>
<td width="35%" height=150>
<a href="https://aimerykong.github.io/">
<img src="aimerykong_files/Far3Det.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/abs/2211.13858"><b>Far3Det: Towards Far-Field 3D Detection</b></a>
<br>
<a href="https://www.linkedin.com/in/shubham-gupta94/">S. Gupta</a>,
<a href="https://www.linkedin.com/in/jeet-kanjani-a86062107/">J. Kanjani</a>,
<a href="https://mtli.github.io/">M. Li</a>,
<a href="https://www.linkedin.com/in/francesco-ferroni-44708137/">F. Ferroni</a>,
<a href="https://faculty.cc.gatech.edu/~hays/">J. Hays</a>,
<a href="http://www.cs.cmu.edu/~deva/">D. Ramanan</a>
S. Kong
<br>
In WACV 2023
<br>
[<a href="https://arxiv.org/abs/2211.13858">github</a>]
[<a href="https://arxiv.org/abs/2211.13858">paper</a>]
<br>
</td>




<tr>
<td width="35%" height=150>
<a href="https://linzhiqiu.github.io/papers/leco/">
<img src="aimerykong_files/splashy_NeurIPS2022.jpg" height="110" border="0">
</a><br>
</td>
<td>
<a href="https://linzhiqiu.github.io/papers/leco/">
<b>Continual Learning With an Evolving Class Ontology</b></a>
<br>
<a href="https://linzhiqiu.github.io/">Z. Lin</a>, 
	<a href="https://www.cs.cmu.edu/~dpathak/">D. Pathak</a>, 
	<a href="https://yxw.web.illinois.edu/">Y. Wang</a>, 
        <a href="http://www.cs.cmu.edu/~deva/">D. Ramanan</a>
	S. Kong <br>
In NeurIPS 2022
<br>
[<a href="https://linzhiqiu.github.io/papers/leco/">webpage</a>]
[<a href="https://arxiv.org/abs/2210.04993">paper</a>]
<br>
</td>





<tr>
<td width="35%" height=150>
<a href="https://github.com/neeharperi/LT3D">
<img src="aimerykong_files/splashy_corl2022.jpg" height="110" border="0">
</a><br>
</td>
<td>
<a href="https://github.com/neeharperi/LT3D">
<b>Towards Long Tailed 3D Detection</b></a>
<br>
<a href="http://www.neeharperi.com/">N. Peri</a>, 
		<a href="https://www.achaldave.com/">A. Dave</a>, 
        <a href="http://www.cs.cmu.edu/~deva/">D. Ramanan</a>, S. Kong <br>
In CoRL 2022
<br>
[<a href="https://github.com/neeharperi/LT3D">webpage</a>] [<a href="https://arxiv.org/abs/2211.08691">paper</a>]
<br>
</td>



<tr>
<td width="35%" height=150>
<a href="https://github.com/Jamie725/RGBT-detection">
<img src="aimerykong_files/ECCV22-RGBT-Det.jpg" height="110" border="0">
</a><br>
</td>
<td>
<a href="https://github.com/Jamie725/RGBT-detection">
<b>Multimodal Object Detection via Probabilistic Ensembling</b></a>
<br>
<a href="https://sites.google.com/media.ee.ntu.edu.tw/yitingchen">Y-T Chen</a><sup>*</sup>, 
		<a href="https://www.linkedin.com/in/jinghao-shi-82371919b/">J. Shi</a><sup>*</sup>, 
		<a href="https://www.linkedin.com/in/zelinye/">Z. Ye</a><sup>*</sup>, 
		<a href="https://www.ri.cmu.edu/ri-people/christoph-mertz/">C. Mertz</a>,
        <a href="http://www.cs.cmu.edu/~deva/">D. Ramanan</a>, S. Kong <br>
In ECCV 2022 (<font color=ff3399>oral presentation</font>)
<br>
[<a href="https://mscvprojects.ri.cmu.edu/2020teamc/">webpage</a>]
[<a href="https://arxiv.org/abs/2104.02904">paper</a>]
[<a href="https://github.com/Jamie725/RGBT-detection">github</a>]
[<a href="https://youtu.be/VH7826g8u7c">video demo</a>]<br>
</td>


<tr>
<td width="35%" height=150>
<a href="https://github.com/aimerykong/OpenGAN">
<img src="aimerykong_files/PAMI-OpenGAN.jpg" height="120" border="0">
</a><br>
</td>
<td>
<a href="https://github.com/aimerykong/OpenGAN">
<b>OpenGAN: Open-Set Recognition via Open Data Generation</b></a>
<br>
S. Kong, <a href="http://www.cs.cmu.edu/~deva/">D. Ramanan</a> <br>
IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2022<br>
[<a href="https://github.com/aimerykong/OpenGAN">webpage</a>]
[<a href="https://ieeexplore.ieee.org/document/9799769">paper</a>]
[<a href="https://github.com/aimerykong/aimerykong.github.io/raw/main/OpenGAN_files/PAMI_OpenGAN_accepted_version.pdf">pdf</a> (18MB)]
[<a href="https://github.com/aimerykong/OpenGAN">code</a>]
[<a href="./img/OpenGAN_poster.pdf">poster</a>]
[<a href="./img/OpenGAN_slides.pdf">slides</a>]
[<a href="https://youtu.be/CNYqYXyUHn0">watch 12min video presentation</a>]<br>
Journal version of <a href="https://arxiv.org/abs/2104.02939">ICCV 2021 paper</a> (<font color=ff3399>Marr Prize / Best Paper Honorable Mention</font>)
</td>



<tr>
<td width="35%" height=150>
<a href="https://github.com/aimerykong/pollenDetClsSystem">
<img src="aimerykong_files/MEE2022.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13917"><b>Automated Identification of Diverse Neotropical Pollen Samples using Convolutional Neural Networks</b></a>
<br>
<a href="http://publish.illinois.edu/punyasena/">S. Punyasena</a><sup>*</sup>,
<a href="https://www.linkedin.com/in/derek-haselhorst-54aa97219/">D. Haselhorst</a><sup>*</sup>,  
S. Kong<sup>*</sup>, 
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>, 
<a href="https://profiles.si.edu/display/nMorenoE872006">J. Moreno</a>
<br>
Methods in Ecology and Evolution, 2022<br>
[<a href="https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13917">paper</a>]
[<a href="https://www.ics.uci.edu/~skong2/pollen_BIC.html">webpage</a>]
[<a href="https://github.com/aimerykong/pollenDetClsSystem">code</a>]
<br>
</td>





<tr>
<td width="35%" height=150>
<a href="https://github.com/ShadeAlsha/LTR-weight-balancing">
<img src="aimerykong_files/CVPR21-LTR.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://github.com/ShadeAlsha/LTR-weight-balancing"><b>Long-Tailed Recognition via Weight Balancing</b></a>
<br>
<a href="https://www.linkedin.com/in/salshamm/">S. Alshammari</a>, 
	<a href="https://yxw.web.illinois.edu">Y. Wang</a>,
        <a href="http://www.cs.cmu.edu/~deva/">D. Ramanan</a>, S Kong
<br>
In CVPR, 2022<br>
[<a href="https://github.com/ShadeAlsha/LTR-weight-balancing">github</a>]
[<a href="https://arxiv.org/abs/2203.14197">paper</a>]
<br>
</td>







<tr>
<td width="35%" height=150>
<a href="https://github.com/aimerykong/OpenGAN">
<img src="aimerykong_files/PAMI-OpenGAN.jpg" height="120" border="0">
</a><br>
</td>
<td>
<a href="https://github.com/aimerykong/OpenGAN">
<b>OpenGAN: Open-Set Recognition via Open Data Generation</b></a>
<br>
S. Kong, <a href="http://www.cs.cmu.edu/~deva/">D. Ramanan</a> <br>
In ICCV, 2021  (<font color=ff3399>Marr Prize / Best Paper Honorable Mention</font>)
<br>
[<a href="https://github.com/aimerykong/OpenGAN">webpage</a>]
[<a href="https://arxiv.org/abs/2104.02939">paper</a>]
[<a href="https://github.com/aimerykong/OpenGAN">code</a>]
[<a href="./img/OpenGAN_poster.pdf">poster</a>]
[<a href="./img/OpenGAN_slides.pdf">slides</a>]
[<a href="https://youtu.be/CNYqYXyUHn0">watch 12min video presentation</a>]<br>
See journal version published by PAMI [<a href="https://github.com/aimerykong/aimerykong.github.io/raw/main/OpenGAN_files/PAMI_OpenGAN_accepted_version.pdf">pdf</a> (18MB)] 
</td>






<tr>
<td width="35%" height=150>
<a href="https://www.ics.uci.edu/~yunhaz5/cvpr2021/cpp.html">
<img src="aimerykong_files/CVPR21-campose.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://www.ics.uci.edu/~yunhaz5/cvpr2021/cpp.html"><b>Camera Pose Matters: Improving Depth Prediction by Mitigating Pose Distribution Bias</b></a>
<br>
<a href="https://www.ics.uci.edu/~yunhaz5/">Y. Zhao</a>,
S. Kong,
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>
<br>
In CVPR, 2021  (<font color=ff3399>oral presentation</font>)
<br>
[<a href="https://www.ics.uci.edu/~yunhaz5/cvpr2021/cpp.html">webpage</a>]
[<a href="https://arxiv.org/abs/2007.03887">arxiv</a>]
[<a href="https://github.com/yzhao520/CPP">github</a>]
[<a href="https://www.ics.uci.edu/~yunhaz5/arXiv/CPP_poster.pdf">slides</a>]
<br>
</td>





<tr>
<td width="35%" height=150>
<a href="https://github.com/aimerykong/deepPollen">
<img src="aimerykong_files/PNAS20.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://www.pnas.org/content/117/45/28496"><b>Improving the Taxonomy of Fossil Pollen using Convolutional Neural Networks and Superresolution Microscopy</b></a>
<br>
<a href="https://www.linkedin.com/in/ingrid-romero-215347193/">I. Romero</a>, 
S. Kong,
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>,
<a href="https://stri.si.edu/scientist/carlos-jaramillo">C. Jaramillo</a>, 
M. Urban, 
<a href="https://sites.google.com/a/mst.edu/francisca-oboh-ikuenobe/">F. Oboh-Ikuenobe</a>, 
<a href="https://scholar.google.com/citations?hl=en&user=askuBtwAAAAJ&view_op=list_works">C. D'Apolito</a>, 
<a href="http://publish.illinois.edu/punyasena/">S. Punyasena</a>
<br>
Proceedings of the National Academy of Sciences of the USA (PNAS), 2020 (<a href="https://www.nsf.gov/discoveries/disc_summ.jsp?cntn_id=301568&org=NSF&from=news"><font color=ff3399>Featured by NSF news</font></a>)
<br>
[<a href="https://doi.org/10.1073/pnas.2007324117">paper</a>] 
[<a href="https://github.com/aimerykong/deepPollen">code</a>]
<br>
</td>




<tr>
<td width="35%" height=150>
<a href="https://github.com/yzhao520/ARC">
<img src="aimerykong_files/CVPR20-ARC.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://github.com/yzhao520/ARC"><b>Domain Decluttering: Simplifying Images to Mitigate
Synthetic-Real Domain Shift and Improve Depth Estimation</b></a>
<br>
<a href="https://www.ics.uci.edu/~yunhaz5/">Y. Zhao</a>, 
S. Kong, 
<a href="https://research.dshin.org/">D. Shin</a>, 
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>,
<br>
In CVPR, 2020
<br>
[<a href="https://www.ics.uci.edu/~yunhaz5/cvpr2020/domain_decluttering.html">webpage</a>]
[<a href="https://arxiv.org/abs/2002.12114">arxiv</a>]
[<a href="https://www.ics.uci.edu/~yunhaz5/cvpr2020/4343-slides.pdf">slides</a>]
[<a href="https://github.com/yzhao520/ARC">github</a>]
<br>
</td>


<tr>
<td width="35%" height=150>
<a href="https://github.com/DerrickWanglf/Celeganser-Automated-Analysis-of-Nematode-Morphology-and-Age">
<img src="aimerykong_files/CVMI20-Celeganser.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://github.com/DerrickWanglf/Celeganser-Automated-Analysis-of-Nematode-Morphology-and-Age"><b>
Celeganser: Automated Analysis of Nematode Morphology and Age</b></a>
<br>
<a href="https://derrickwanglf.github.io/">Linfeng Wang</a>,
S. Kong,
<a href="https://zplab.wustl.edu">Zachary Pincus</a>,
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>,
<br>
In <a href="https://cvmi2020.github.io/index.html">CVMI</a>@<a href="http://cvpr2020.thecvf.com/">CVPR</a>, 2020
<br>
[<a href="https://github.com/DerrickWanglf/Celeganser-Automated-Analysis-of-Nematode-Morphology-and-Age">webpage</a>]
[<a href="https://arxiv.org/abs/2005.04884">arxiv</a>]
[<a href="https://github.com/DerrickWanglf/Celeganser-Automated-Analysis-of-Nematode-Morphology-and-Age">github</a>]
<br>
</td>







<tr>
<td width="35%" height=150>
<a href="https://github.com/aimerykong/Dimensional-Emotion-Analysis-of-Facial-Expression">
<img src="aimerykong_files/neurocomputing-facialexpression.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://github.com/aimerykong/Dimensional-Emotion-Analysis-of-Facial-Expression"><b>
Fine-Grained Facial Expression Analysis Using Dimensional Emotion Model</b></a>
<br>
<a href="https://umdearborn.edu/users/fezhou">F. Zhou</a><sup>*</sup>,
S. Kong<sup>*</sup>,
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>,
T. Chen, 
B. Lei
<br>
<a href="https://arxiv.org/abs/1805.01024">Neurocomputing</a>, 2020
<br>
[<a href="https://github.com/aimerykong/Dimensional-Emotion-Analysis-of-Facial-Expression">github</a>]
[<a href="https://arxiv.org/abs/1805.01024">arxiv</a>]
[<a href="https://youtu.be/F8cCXGxGjFQ">demo</a>]
[<a href="https://drive.google.com/drive/folders/1CVP12ex9q93PsTeredR2nvrMslNubLLk">models</a>]
<br>
</td>



<tr>
<td width="35%" height=150>
<a href="https://arxiv.org/abs/2006.11747">
<img src="aimerykong_files/WSRA.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/abs/2006.11747"><b>
Weakly-Supervised Temporal-Language Association with Referring Attention</b></a>
<br>
<a href="https://scholar.google.com/citations?user=fHWXpq4AAAAJ&hl=en#">Z. Fang</a>,
S. Kong,
Z. Wang, 
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>,
<a href="https://yezhouyang.engineering.asu.edu/">Y. Yang</a>
<br>
<a href="https://arxiv.org/abs/2006.11747">arXiv:2006.11747</a>, 2020
<br>
[<a href="https://arxiv.org/abs/2006.11747">arxiv</a>]
<br>
</td>



<tr>
<td width="35%" height=150>
<a href="https://github.com/jacobswan1/MTG-pytorch">
<img src="aimerykong_files/CVPR19-counterfactual.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://github.com/jacobswan1/MTG-pytorch"><b>
Modularized Textual Grounding for Counterfactual Resilience</b></a>
<br>
<a href="https://scholar.google.com/citations?user=fHWXpq4AAAAJ&hl=en#">Z. Fang</a>,
S. Kong,
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>,
<a href="https://yezhouyang.engineering.asu.edu/">Y. Yang</a>
<br>
In CVPR, 2019
<br>
[<a href="https://arxiv.org/abs/1904.03589">paper</a>]
[<a href="https://github.com/jacobswan1/MTG-pytorch">github</a>]
<br>
</td>




<tr>
<td width="30%" height=120>
<a href="https://github.com/aimerykong/Pixel-Attentional-Gating">
<img src="aimerykong_files/WACV19-PAG.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://github.com/aimerykong/Pixel-Attentional-Gating"><b>
Pixel-wise Attentional Gating for Scene Parsing</b></a>
<br>
S. Kong,
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>
<br>
in WACV, 2019
<br>
[<a href="https://arxiv.org/abs/1805.01556">arxiv</a>]
[<a href="https://github.com/aimerykong/Pixel-Attentional-Gating">github</a>]
[<a href="https://www.ics.uci.edu/~skong2/slides/20180514_AIML_UCI.pdf">slides</a>]
[<a href="http://www.robustvision.net/leaderboard.php?benchmark=depth">ROB Entry of Depth Est.</a>]
[<a href="http://robustvision.net/leaderboard.php?benchmark=semantic">ROB Entry of Segm.</a>]
<br>
</td>



<tr>
<td width="35%" height=150>
<a href="https://arxiv.org/abs/2006.11747">
<img src="aimerykong_files/icon_mgpff_small_dog.gif" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://arxiv.org/abs/2006.11747"><b>
Multigrid Predictive Filter Flow for Unsupervised Learning on Videos</b></a>
<br>
S. Kong,
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>
<br>
<a href="http://arxiv.org/abs/1904.01693">arXiv:1904.01693</a>, 2019
<br>
[<a href="https://www.ics.uci.edu/~skong2/mgpff.html">webpage</a>]
[<a href="http://arxiv.org/abs/1904.01693">arxiv</a>]
[<a href="https://github.com/aimerykong/predictive-filter-flow/tree/master/mgPFF_video">github</a>]
[<a href="https://github.com/aimerykong/predictive-filter-flow/blob/master/mgPFF_video/demo01_videoSegTrack.ipynb">demo</a>]
[<a href="https://docs.google.com/presentation/d/1VcA794mt0ukg2ojUnOzbFL8HzpRjEbRp4HXULAIp13Y/edit?usp=sharing">slides</a>]
<br>
</td>



<tr>
<td width="35%" height=150>
<a href="https://github.com/aimerykong/Recurrent-Pixel-Embedding-for-Instance-Grouping">
<img src="aimerykong_files/CVPR18-pixelgrouping.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://github.com/aimerykong/Recurrent-Pixel-Embedding-for-Instance-Grouping"><b>
Recurrent Pixel Embedding for Instance Grouping</b></a>
<br>
S. Kong,
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>
<br>
In CVPR, 2018 (<font color=ff3399>spotlight presentation</font>)
<br>
[<a href="https://arxiv.org/abs/1712.08273">arxiv</a>]
[<a href="https://drive.google.com/drive/folders/1K2bCmz_mldIhV1e3hCbtBrARZR_0bylm?usp=sharing">models</a>]
[<a href="https://github.com/aimerykong/Recurrent-Pixel-Embedding-for-Instance-Grouping">github</a>]
[<a href="./slides/pixel_embedding_for_grouping_poster.pdf">poster</a>]
[<a href="./slides/pixel_embedding_for_grouping_public_version.pdf">slides</a>]
<br>
</td>




<tr>
<td width="35%" height=150>
<a href="https://github.com/aimerykong/Recurrent-Scene-Parser-with-Perspective-Estimation-in-the-loop">
<img src="aimerykong_files/CVPR18-perspective-in-the-loop.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://github.com/aimerykong/Recurrent-Scene-Parser-with-Perspective-Estimation-in-the-loop"><b>
Recurrent Scene Parsing with Perspective Understanding in the Loop</b></a>
<br>
S. Kong,
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>
<br>
In CVPR, 2018
<br>
[<a href="https://arxiv.org/abs/1705.07238">paper</a>]
[<a href="https://github.com/aimerykong/Recurrent-Scene-Parser-with-Perspective-Estimation-in-the-loop">github</a>]
<br>
</td>




<tr>
<td width="35%" height=150>
<a href="https://github.com/aimerykong/predictive-filter-flow">
<img src="aimerykong_files/FilterFlow.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://github.com/aimerykong/predictive-filter-flow"><b>
Image Reconstruction with Predictive Filter Flow</b></a>
<br>
S. Kong,
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>
<br>
<a href="https://arxiv.org/abs/1811.11482">arXiv:1811.11482</a>, 2018.
<br>
[<a href="https://arxiv.org/abs/1811.11482">paper</a>]
[<a href="https://www.ics.uci.edu/~skong2/pff.html">webpage</a>]
[<a href="https://www.ics.uci.edu/~skong2/slides/kf_ff_arxiv2018.pdf">high-res paper (44MB)</a>]
[<a href="https://github.com/aimerykong/predictive-filter-flow">github</a>]
<br>
</td>





<tr>
<td width="35%" height=150>
<a href="https://github.com/aimerykong/Low-Rank-Bilinear-Pooling">
<img src="aimerykong_files/CVPR17-LRBP.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://github.com/aimerykong/Low-Rank-Bilinear-Pooling"><b>
Low-rank Bilinear Pooling for Fine-Grained Classification</b></a>
<br>
S. Kong,
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>
<br>
In CVPR, 2017.
<br>
[<a href="https://arxiv.org/abs/1611.05109">paper</a>]
[<a href="https://github.com/aimerykong/Low-Rank-Bilinear-Pooling">github</a>]
<br>
</td>







<tr>
<td width="35%" height=150>
<a href="https://github.com/aimerykong/deepImageAestheticsAnalysis">
<img src="aimerykong_files/ECCV16-aesthetics.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://github.com/aimerykong/deepImageAestheticsAnalysis"><b>
Photo Aesthetics Ranking Network with Attributes and Content Adaptation</b></a>
<br>
S. Kong,
<a href="https://scholar.google.com/citations?user=pViZYwIAAAAJ&hl=en">X. Shen</a>, 
<a href="https://scholar.google.com/citations?user=R0bnqaAAAAAJ&hl=en">Z. Lin</a>,
<a href="https://scholar.google.com/citations?user=RTbYeUcAAAAJ&hl=en">R. Mech</a>,
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>
<br>
In ECCV, 2016.
<br>
[<a href="http://arxiv.org/abs/1606.01621">paper</a>]
[<a href="https://github.com/aimerykong/deepImageAestheticsAnalysis">github</a>]
[<a href="https://docs.google.com/document/d/1HIAvnKbrEAH-lxW7lABKFe81-_LAfHajJKICuwRz7Tc/edit?usp=sharing">AMT instruction</a>]
[<u>patent filed</u>]
<br>
</td>






<tr>
<td width="35%" height=150>
<a href="https://github.com/aimerykong/PatchMatchingForPollenIdentification">
<img src="aimerykong_files/CVPR15-CVMI.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://github.com/aimerykong/PatchMatchingForPollenIdentification"><b>
Spatially Aware Dictionary Learning and Coding for Fossil Pollen Identification</b></a>
<br>
S. Kong,
<a href="http://publish.illinois.edu/punyasena/">S. Punyasena</a>,
<a href="https://www.ics.uci.edu/~fowlkes/">C. Fowlkes</a>
<br>
In CVMI@CVPR, 2016
<br>
[<a href="https://github.com/aimerykong/PatchMatchingForPollenIdentification">github</a>]
[<a href="http://arxiv.org/abs/1605.00775">paper</a>]
<br>
</td>



<tr>
<td width="30%" height=150>
<a href="https://ieeexplore.ieee.org/document/7069202?arnumber=7069202&sortType%3Dasc_p_Sequence%26filter%3DAND(p_Publication_Number:83)%26rowsPerPage%3D50=">
<img src="aimerykong_files/TIP14.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://ieeexplore.ieee.org/document/7069202?arnumber=7069202&sortType%3Dasc_p_Sequence%26filter%3DAND(p_Publication_Number:83)%26rowsPerPage%3D50="><b>
Modeling Neuron Selectivity over Simple Mid-Level Features for Image Classification</b></a>
<br>
S. Kong,
<a href="https://scholar.google.com/citations?user=X55lT7kAAAAJ&hl=en">Z. Jiang</a>, 
<a href="https://cse.hkust.edu.hk/~qyang/">Q. Yang</a>
<br>
IEEE Transactions on Image Processing, 2015.
<br>
[<a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7069202&sortType%3Dasc_p_Sequence%26filter%3DAND%28p_Publication_Number%3A83%29%26rowsPerPage%3D50">paper</a>]
<br>
</td>





<tr>
<td width="35%" height=150>
<a href="http://www.aaai.org/ocs/index.php/WS/AAAIW14/paper/download/8725/8364">
<img src="aimerykong_files/AAAI14.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="http://www.aaai.org/ocs/index.php/WS/AAAIW14/paper/download/8725/8364"><b>
Saliency Detection within a Deep Convolutional Architecture</b></a>
<br>
<a href="https://scholar.google.com/citations?user=nGbj-Q4AAAAJ&hl=en">Y. Lin</a>, 
S. Kong,
<a href="https://scholar.google.com/citations?user=AkRWtMUAAAAJ&hl=en">D. Wang</a>, 
<a href="https://scholar.google.com/citations?user=1RD7UJAAAAAJ&hl=en">Y. Zhuang</a>
<br>
In AAAI'14 Workshop on Cognitive Computing for Augmented Human Intelligence, 2014
<br>
<a href="http://www.aaai.org/ocs/index.php/WS/AAAIW14/paper/download/8725/8364">[paper]</a>
<br>
</td>




<tr>
<td width="35%" height=150>
<a href="http://www.sciencedirect.com/science/article/pii/S0031320313003245">
<img src="aimerykong_files/PR13.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="http://www.sciencedirect.com/science/article/pii/S0031320313003245"><b>
A Classification-Oriented Dictionary Learning Model: Explicitly Learning the Particularity and Commonality Across Categories</b></a>
<br>
<a href="https://scholar.google.com/citations?user=AkRWtMUAAAAJ&hl=en">D. Wang</a><sup>*</sup>,
S. Kong<sup>*</sup>
<br>
Pattern Recognition, 2013
<br>
<a href="http://www.sciencedirect.com/science/article/pii/S0031320313003245">[paper]</a>
<a href="http://aimerykong.weebly.com/uploads/1/2/8/7/12871977/copardic_release_version_1.rar">[code]</a>
<br>
</td>




<tr>
<td width="35%" height=150>
<a href="http://www.ecmlpkdd2013.org/wp-content/uploads/2013/07/21.pdf">
<img src="aimerykong_files/ECMLPKDD13.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="http://www.ecmlpkdd2013.org/wp-content/uploads/2013/07/21.pdf"><b>
Learning Exemplar-Represented Manifolds in Latent Space for Classification</b></a>
<br>
S. Kong,
<a href="https://scholar.google.com/citations?user=AkRWtMUAAAAJ&hl=en">D. Wang</a>
<br>
In ECML/PKDD, 2013
<br>
<a href="http://www.ecmlpkdd2013.org/wp-content/uploads/2013/07/21.pdf">[paper]</a>
<a href="http://aimerykong.weebly.com/uploads/1/2/8/7/12871977/emlsc.zip">[code]</a>
<br>
</td>





<tr>
<td width="35%" height=150>
<a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885613001509">
<img src="aimerykong_files/IVC13.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885613001509"><b>
Integration of Multi-Feature Fusion and Dictionary Learning for Face Recognition</b></a>
<br>
<a href="https://scholar.google.com/citations?user=AkRWtMUAAAAJ&hl=en">D. Wang</a>,
<a href="https://scholar.google.com/citations?user=PYf9Mi4AAAAJ&hl=en">X. Wang</a>,
S. Kong
<br>
Image and Vision Computing (IVC), 2013
<br>
<a href="http://www.sciencedirect.com/science/article/pii/S0262885613001509">[paper]</a>
<a href="http://aimerykong.weebly.com/uploads/1/2/8/7/12871977/multimodal.rar">[code]</a>
<br>
</td>





<tr>
<td width="35%" height=150>
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6553710&casa_token=c2VOuTB94nAAAAAA:th-WG1dls0p_vIS3t8-q0QSeKufxX5iKTMIvQrRK_6X51j9GR_kab3RZsEaoYy5HA0qPoQGP&tag=1">
<img src="aimerykong_files/FG13a.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6553710&casa_token=c2VOuTB94nAAAAAA:th-WG1dls0p_vIS3t8-q0QSeKufxX5iKTMIvQrRK_6X51j9GR_kab3RZsEaoYy5HA0qPoQGP&tag=1"><b>
Learning Individual-Specific Dictionaries with Fused Multiple Features for Face Recognition</b></a>
<br>
S. Kong,
<a href="https://scholar.google.com/citations?user=AkRWtMUAAAAJ&hl=en">D. Wang</a>
<br>
In IEEE conference series on Automatic Face and Gesture Recognition (FG), 2013
<br>
<a href="http://www.cs.zju.edu.cn/people/wangdh/papers/dictionary_fusion_FG2013.pdf">[paper]</a>
<br>
</td>





<tr>
<td width="35%" height=150>
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6553718">
<img src="aimerykong_files/FG13b.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6553718"><b>
Multiple Feature Fusion for Face Recognition</b></a>
<br>
S. Kong,
<a href="https://scholar.google.com/citations?user=PYf9Mi4AAAAJ&hl=en">X. Wang</a>,
<a href="https://scholar.google.com/citations?user=AkRWtMUAAAAJ&hl=en">D. Wang</a>,
<a href="https://scholar.google.com.hk/citations?user=XJLn4MYAAAAJ&hl=en">F. Wu</a>
<br>
In IEEE conference series on Automatic Face and Gesture Recognition (FG), 2013
<br>
<a href="http://www.cs.zju.edu.cn/people/wangdh/papers/multiple_feature_fusion_FG2013.pdf">[paper]</a>
<a href="http://aimerykong.weebly.com/uploads/1/2/8/7/12871977/multimodal.rar">[code]</a>
<br>
</td>





<tr>
<td width="35%" height=150>
<a href="http://link.springer.com/chapter/10.1007%2F978-3-642-33718-5_14">
<img src="aimerykong_files/ECCV12-dict.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="http://link.springer.com/chapter/10.1007%2F978-3-642-33718-5_14"><b>
A Dictionary Learning Approach for Classification: Separating the Particularity and the commonality</b></a>
<br>
S. Kong,
<a href="https://scholar.google.com/citations?user=AkRWtMUAAAAJ&hl=en">D. Wang</a>
<br>
In ECCV, 2012
<br>
<a href="http://link.springer.com/chapter/10.1007%2F978-3-642-33718-5_14">[paper]</a>
<a href="http://aimerykong.weebly.com/uploads/1/2/8/7/12871977/copardic_release_version_1.rar">[code]</a>
<br>
</td>






<tr>
<td width="35%" height=150>
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6460248">
<img src="aimerykong_files/ICPR12a.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6460248"><b>
A Multi-task Learning Strategy for Unsupervised Clustering via Explicitly Separating the Commonality</b></a>
<br>
S. Kong,
<a href="https://scholar.google.com/citations?user=AkRWtMUAAAAJ&hl=en">D. Wang</a>
<br>
In ICPR, 2012
<br>
<a href="http://www.cs.zju.edu.cn/people/wangdh/papers/draft_ICPR2012_multiTask_full.pdf">[paper]</a>
<br>
</td>




<tr>
<td width="35%" height=150>
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6460351">
<img src="aimerykong_files/ICPR12b.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6460248"><b>
Transfer heterogeneous unlabeled data for unsupervised clustering</b></a>
<br>
S. Kong,
<a href="https://scholar.google.com/citations?user=AkRWtMUAAAAJ&hl=en">D. Wang</a>
<br>
In ICPR, 2012
<br>
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6460351">[paper]</a>
<br>
</td>







<tr>
<td width="35%" height=150>
<a href="https://link.springer.com/article/10.1007/s00138-012-0463-z">
<img src="aimerykong_files/MVA2012.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="https://link.springer.com/article/10.1007/s00138-012-0463-z"><b>
Learning class-specific dictionaries for digit recognition from spherical surface of a 3D ball</b></a>
<br>
<a href="https://scholar.google.com/citations?user=AkRWtMUAAAAJ&hl=en">D. Wang</a>,
S. Kong
<br>
Machine Vision and Applications (MVA), 2012
<br>
<a href="http://link.springer.com/article/10.1007%2Fs00138-012-0463-z">[paper]
<a href="http://www.cs.zju.edu.cn/people/wangdh/download/new_image_3.30.rar"> [SingleBall_dataset (288MB)]</a>
<a href="http://www.cs.zju.edu.cn/people/wangdh/download/mult-ball_image_dataset.rar">[MultiBall_dataset (121MB)]</a>
<br>
</td>




<tr>
<td width="35%" height=150>
<a href="http://www.sciencedirect.com/science/article/pii/S0167865512001985">
<img src="aimerykong_files/PRL2012.jpg" height="120"  border="0">
</a><br>
</td>
<td>
<a href="http://www.sciencedirect.com/science/article/pii/S0167865512001985"><b>
Feature Selection from High-Order Tensorial Data via Sparse Decomposition</b></a>
<br>
<a href="https://scholar.google.com/citations?user=AkRWtMUAAAAJ&hl=en">D. Wang</a>,
S. Kong
<br>
Pattern Recognition Letters, 2012
<br>
<a href="http://www.sciencedirect.com/science/article/pii/S0167865512001985"> [paper]</a>
<a href="http://aimerykong.weebly.com/uploads/1/2/8/7/12871977/shopca_version1.zip">[code]</a>
<br>
</td>



</tbody>
</table>




<h3>Abstract/Workshop Papers</h3>
<ul>

<li>
<div class="publication">
<p> S. W. Punyasena, M. É. Adaïmé, and S. Kong, "<font color=#AF7817>Deep learning approaches to the phylogenetic placement of fossil pollen morphotypes</font>", 
	<em>15th International Palynological Congress and 11th International Organization of Palaeobotany Conference</em>, Prague, Czech Republic; May 2024.
<br>
</p>
</div>
</li>


	
<li>
<div class="publication">
<p> S. W. Punyasena, J. T. Feng, S. Puthanveetil Satheesan, and S. Kong, "<font color=#AF7817>Development of a high-throughput fossil pollen analysis pipeline</font>",
	<em>North American Paleontological Convention 2024</em>,  Ann Arbor, MI; June 2024.
<br>
</p>
</div>
</li>
	

<li>
<div class="publication">
<p> M. É. Adaïmé, S. Kong, M. A. Urban, and S. W. Punyasena, 
	"<font color=#AF7817>Reconstructing the diversity dynamics of Late Quaternary East African grasslands using superresolution imaging of fossil Poaceae pollen and deep learning</font>", 
	<em>North American Paleontological Convention 2024</em>,  Ann Arbor, MI; June 2024.
<br>
</p>
</div>
</li>

	
<li>
<div class="publication">
<p> M. É. Adaïmé, S. Kong, and S. W. Punyasena, 
	"<font color=#AF7817>Using machine learning and superresolution imaging of fossil Poaceae to reconstruct the biodiversity dynamics of Late Quaternary East African grasslands</font>",  
	<em>41st Annual Midcontinental Paleobotany Meeting</em>,  Field Museum, Chicago, IL; April 2024.
<br>
</p>
</div>
</li>



<li>
<div class="publication">
<p> B. Lloyd, M. É Adaïmé, S. W. Punyasena, T. Gallaher, S. Kong, and C. E. Strömberg, 
	"<font color=#AF7817>A deep learning approach to the taxonomic classification of grass silica short cell phytoliths</font>",  
	<em>41st Annual Midcontinental Paleobotany Meeting</em>,  Field Museum, Chicago, IL; April 2024.
<br>
</p>
</div>
</li>

	
	
<li>
<div class="publication">
<p>Jia Shi, Gautam Gare, Jinjin Tian, Siqi Chai, Zhiqiu Lin, Arun Vasudevan, Di Feng, Francesco Ferroni, Shu Kong, Deva Ramanan 
"<font color=#AF7817>LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies</font>",
<em><a href="https://openreview.net/forum?id=7CUutNeDDg">NeurIPS Workshop on DistShift</a></em>, 2023
</p>
</div>
</li>



<li>
<div class="publication">
<p> M. É. Adaïmé, S. Kong, and S. W. Punyasena, "<font color=#AF7817>Deep learning approaches to the phylogenetic placement of extinct pollen morphotypes</font>",  
	<em>Geological Society of America GSA Connects (2023 Annual Meeting)</em>,  Pittsburgh, PA; October 2023.
<br>
</p>
</div>
</li>


	

<li>
<div class="publication">
<p> M-E. Adaime, S. Kong, and S.W. Punyasena, "<font color=#AF7817>Deep metric learning and the phylogenetic placement of novel fossil pollen</font>", 
	<em>39th Annual Midcontinental Paleobotany Meeting</em>, Oak Spring Foundation, Upperville, VA; May 2022.
<br>
</p>
</div>
</li>

	
	

<li>
<div class="publication">
<p> M-E. Adaime, S. Kong, and S.W. Punyasena, "<font color=#AF7817>Phylogenetically-informed computer vision methods for fossil pollen classification: ecological and evolutionary implications</font>", 
	<em>38th Annual Midcontinental Paleobotany Meeting</em>, vitural, hosted by University of California Berkley / University of Washington / University of Wyoming; June 2021.
<br>
</p>
</div>
</li>
	
	
<li>
<div class="publication">
<p> M-E. Adaime, S. Kong, and S.W. Punyasena, "<font color=#AF7817>Phylogenetically-informed computer vision methods for fossil pollen classification: ecological and evolutionary implications</font>", 
	<em>38th Annual Midcontinental Paleobotany Meeting</em>, vitural, hosted by University of California Berkley / University of Washington / University of Wyoming; June 2021.
<br>
</p>
</div>
</li>

	
	
<li>
<div class="publication">
<p>Zhiyuan Fang, Shu Kong, Charless Fowlkes, Yezhou Yang
"<font color=#AF7817> Modularized Textual Grounding for Counterfactual Resilience</font>",
<em><a href="http://languageandvision.com">Language And Vision workshop joint with CVPR</a></em>, 2019.
</p>
</div>
</li>



<li>
<div class="publication">
<p> I.C. Romero, S. Kong, C.C. Fowlkes, S.W. Punyasena, 
	"<font color=#AF7817>Identification of the Cenozoic pollen morphospecies Striatopollis catatumbus (Amherstieae, Fabaceae) using convolutional neural nets</font>", 
	<em>3rd Annual Digital Data in Biodiversity Research Conference</em>, New Haven, CT; June 2019.
<br>
</p>
</div>
</li>

	

<li>
<div class="publication">
<p>Surangi W. Punyasena,  <b>Shu Kong</b>, Charless C. Fowlkes
"<font color=#AF7817>Improving the taxonomic accuracy and precision of fossil pollen identifications</font>",  
<em><a href="https://napc2019.ucr.edu/">North American Paleontological Convention, Riverside, USA</a></em>, 2019.
</p>
</div>
</li>




<li>
<div class="publication">
<p>Ingrid Romero,  <b>Shu Kong</b>, Charless C. Fowlkes, Michael A. Urban, Surangi W. Punyasena, 
	"<font color=#AF7817>Automated Neotropical Fossil Pollen Fabaceae Analysis Using Convolutional Neural Networks</font>",  <em>GSA Annual Meeting in Indianapolis, Indiana, USA</em>, 2018.
</p>
</div>
</li>




<li>
<div class="publication">
<p>Zhiyuan Fang,  <b>Shu Kong</b>, Tianshu Yu, Yezhou Yang, "<font color=#AF7817>Weakly Supervised Attention Learning for Textual Phrases Grounding</font>",  
	<em><a href="http://languageandvision.com/">Language and Vision Workshop</a> jointwith CVPR</em>, 2018.
</p>
</div>
</li>



<li>
<div class="publication">
<p> <b>Shu Kong</b>, Charless C. Fowlkes, "<font color=#AF7817>Low-rank Bilinear Pooling for Fine-Grained Classification</font>", 
	<em><a href="https://sites.google.com/view/fgvc4/">the Fourth Workshop on Fine-grained Visual Categorization</a> joint with CVPR</em>, 2017.
</p>
</div>
</li>


<li>
<div class="publication">
<p> <b>Shu Kong</b>, Charless C. Fowlkes, "<font color=#AF7817>Recurrent Scene Parsing with Perspective Understanding in the Loop</font>", 
	<em><a href="https://sites.google.com/view/socalml17/home"> Southern California Machine Learning Symposium</a></em>, 2017.
</p>
</div>
</li>

	
<li>
<div class="publication">
<p> I. Romero, S. Kong, C.C. Fowlkes, M.A. Urban, C. D’Apolito, C. Jaramillo, F.E. Oboh-Ikuenobe, F.E., and S.W. Punyasena, "<font color=#AF7817>Novel morphological analysis of a fossil Fabaceae pollen type, Striatopollis catatumbus (tribe Detariae)</font>",  <em>Geological Society of America Abstracts with Programs (2017 Annual Meeting)</em>, doi: 10.1130/abs/2017AM-305572； Seattle, WA; October 2017.
<br>
</p>
</div>
</li>
	

<li>
<div class="publication">
<p> I.C. Romero, S. Kong, C.C. Fowlkes, M.A. Urban, C.A. D’Apolito, C. Jaramillo, F. Oboh-Ikuenobe, and S.W. Punyasena, "<font color=#AF7817>Cenozoic biogeography of Striatopollis catatumbus (Fabaceae – Detariae)</font>",  <em>AASP-The Palynological Society</em>, Nottingham, England; September 2017.
<br>
</p>
</div>
</li>


<li>
<div class="publication">
<p> Derek S. Haselhorst, <b>Shu Kong</b>, Charless C. Fowlkes, J. Enrique Moreno, David K. Tcheng, Surangi W. Punyasena, 
	"<font color=#AF7817>Automating tropical pollen counts using convolutional neural nets: from image acquisition to identification</font>",  
	<em>the iDigBio inaugural conference</em>, Ann Arbor, MI; June 2017.
<br>
</p>
</div>
</li>

	
<li>
<div class="publication">
<p> Surangi W. Punyasena, <b>Shu Kong</b>, Charless C. Fowlkes, and Stephen P. Jackson, 
	"<font color=#AF7817>Reconstructing the extinction dynamics of Picea critchfieldii - the application of computer vision to fossil pollen analysis
</font>",  <em>the iDigBio inaugural conference</em>, Ann Arbor, MI; June 2017.
<br>
</p>
</div>
</li>

	
<li>
<div class="publication">
<p> <b>Shu Kong</b>, Charless C. Fowlkes "<font color=#AF7817>Low-rank Bilinear Pooling for Fine-Grained Classification</font>",  <em><a href="http://dolcit.cms.caltech.edu/scmls/">Southern California Machine Learning Symposium</a></em>, 2016.
<br>
</p>
</div>
</li>

</ul>



</div>
</div>






</body></html>
