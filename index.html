<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Open+Sans" />
	<link rel="stylesheet" type="text/css" href='static/css/bootstrap.min.css' />
	<link rel="stylesheet" type="text/css" href='static/css/styles.css'/>
	<script src=''></script>
	<title>Shu Kong</title>
	<link rel="icon" href="aimerykong_files/profile2.png" type="img/jpg">
</head>




	
	
	
<body>
	<div class="my-container">
		<h1> Shu Kong </h1>
		<span> Assistant Professor; University of Macau <a href="https://github.com/aimerykong"  target="_blank">[GitHub]</a>  <a href="http://scholar.google.com/citations?user=sm9FdLoAAAAJ&hl=en"  target="_blank">[Google Scholar]</a> </span>

		<div class="btn-group btn-group-justified" role="group" aria-label="Justified button group">
			<a href="index.html" class="btn btn-default" role="button">Home</a>
			<a href="publication.html" class="btn btn-default" role="button">Publication</a>
			<a href="group.html" class="btn btn-default" role="button">Group</a>
			<a href="contact.html" class="btn btn-default" role="button">Contact</a>
    	</div>

    	<div class="content">
    		<div class="row">
				<div class="col-sm-2">
					<img class="img-responsive img-rounded" src="./aimerykong_files/profile_shu.jpg">
				</div>
				<div class="col-sm-10">
        <p style="font-size:16px;font-family:Times"> I am on the faculty of 
		<a href="https://www.fst.um.edu.mo/people/skong/"  target="_blank">FST</a>,  
		<a href="https://www.um.edu.mo/"  target="_blank">University of Macau</a>, 
		leading the <a href="group.html">Computer Vision Lab</a>. 
		I was an Assistant Professor in the Department of <a href="https://engineering.tamu.edu/cse"  target="_blank">CSE</a> at 
		<a href="https://www.tamu.edu/"  target="_blank">Texas A&M University</a>.
		  I did my postdoc training at the <a href="https://www.ri.cmu.edu/"  target="_blank">Robotics Institute</a>, <a href="https://www.cmu.edu/"  target="_blank">CMU</a>, 
		  working with <a href="http://www.cs.cmu.edu/~deva/">Deva Ramanan</a>. 
		  I received my PhD from <a href="http://www.uci.edu/"  target="_blank">UC-Irvine</a>, working with <a href="http://www.ics.uci.edu/~fowlkes/"  target="_blank">Charless Fowlkes</a>.
	</p>
	
	<p style="font-size:16px;font-family:Times">
		My research lies in <strong>Computer Vision</strong> and its interactions with other fields (e.g., ML, NLP, HCI, robotics, and graphics), broad
		applications (e.g., AR/VR, autonomous driving, etc.), and diverse disciplines (e.g., biology, paleoecology, psychology, special education, etc.). 
		My current research focuses on <a href="https://vplow.github.io"  target="_blank"><strong>Visual Perception via Learning in the Open World (VPLOW)</strong></a>. 
		My previous paper on this topic was recognized for <a href="https://arxiv.org/abs/2104.02939"  target="_blank">Best Paper / Marr Prize at ICCV 2021</a>.
		My previous work that applied AI to interdisciplinary research built a high-throughput pollen analysis system,
		which was featured by the National Science Foundation that 
		“<a href="https://beta.nsf.gov/news/modern-computational-tools-open-new-era-fossil-pollen-research"  target="_blank">opens a new era of fossil pollen research</a>”.
	</p>


					
	<!--p style="font-size:16px;font-family:Times">
		My research interests span <strong>Computer Vision</strong> and <strong>applied Machine Learning</strong> (CV/ML), 
		and their applications to autonomous vehicles and research in natural science. 
		My current research in CV/ML focuses on <a href="https://vplow.github.io"><strong>Visual Perception via Learning in the Open World (VPLOW)</strong></a>. 
		My recent paper on this topic was recognized for <a href="https://arxiv.org/abs/2104.02939">Best Paper / Marr Prize at ICCV 2021</a>.
		I also actively apply my algorithms to interdisciplinary research including building a high-throughput pollen analysis system,
		which was featured by the National Science Foundation as that “<a href="https://beta.nsf.gov/news/modern-computational-tools-open-new-era-fossil-pollen-research">opens a new era of fossil pollen research</a>”.
				</p-->
					
	<p style="font-size:16px;font-family:Times">
		<font color="#8B0000">
			I am actively looking for self-motivated PhD students at <a href="https://grs.um.edu.mo/index.php/prospective-students/doctoral-degrees-programmes/"  target="_blank">CIS, FST, University of Macau</a>.
			PhD students will be fully funded with fellowships.
		</font>
			I am NOT actively looking for PhD students at <a href="https://engineering.tamu.edu/cse/admissions-and-aid/graduate-admissions/index.html"  target="_blank">CSE, TAMU</a>.
		
	</p>


<h3 style="font-size:20px;font-family:Times">contact</h3>
<ul style="font-size:15px;font-family:Times;line-height:18px">
<li>gmail: aimerykong [at] gmail [dot] com    [issues related to others; unlikely to reply]</li>
<li>email: skong [at] um [dot] edu [dot] mo   [issues related to the University of Macau]</li>
<li>email: shu [at] tamu [dot] edu   [issues related to TAMU]</li>
</ul>
</div>
</div>



		

<h3 style="font-size:20px;font-family:Times"><font color="red">Call for Participation</font></h3>
<ul style="font-size:15px;font-family:Times;line-height:18px">	
	<li>
	<a href="https://eval.ai/web/challenges/challenge-page/2459/overview"  target="_blank">The 2nd Challenge of Foundational Few-Shot Object Detection @ VPLOW jointly with CVPR'25</a>
	</li>
	
	<li>
	<a href="https://eval.ai/web/challenges/challenge-page/2478/overview"  target="_blank">The 2nd Challenge of Object Instance Detection @ VPLOW jointly with CVPR'25</a>
	</li>
</ul>

		
<h3 style="font-size:20px;font-family:Times">Links</h3>
<ul style="font-size:15px;font-family:Times;line-height:18px">	
	<li>
	<a href="https://vplow.github.io/vplow_5th.html"  target="_blank">The 5th Open World Vision Workshop: Visual Perception via Learning in an Open World (VPLOW) at CVPR'25</a>
	</li>
	<li>
	<a href="https://vplow.github.io/vplow_4th.html"  target="_blank">The 4th Open World Vision Workshop: Visual Perception via Learning in an Open World (VPLOW) at CVPR'24</a>
	</li>
	<li>
	<a href="https://vplow.github.io/vplow_3rd.html"  target="_blank">The 3rd Open World Vision Workshop: Visual Perception via Learning in an Open World (VPLOW) at CVPR'23</a>
	</li>
	<li>
	<a href="https://www.cs.umd.edu/~pulkit/DNOW_workshop_2/"  target="_blank">The 2nd Workshop on Dealing with Novelty in Open Worlds (DNOW) at WACV'23</a>
	</li>
	<li>
	<a href="http://vplow.github.io/vplow.html"  target="_blank">The 2nd Open World Vision Workshop: Visual Perception and Learning in an Open World (VPLOW) at CVPR'22</a>
	</li>
	<li>
	<a href="https://www.cs.umd.edu/~pulkit/DNOW_workshop/"  target="_blank">The 1st Workshop on Dealing with Novelty in Open Worlds (DNOW) at WACV'22</a>
	</li>
	<li>
	<a href="http://vplow.github.io/open-world-vision.html"  target="_blank">The 1st Open World Vision Workshop at CVPR'21</a>
	</li>
</ul>

		
<h3 style="font-size:20px;font-family:Times">Teaching</h3>
<ul style="font-size:15px;font-family:Times;line-height:18px">	
	<li>
		[CISC7401] Advanced Machine Learning [<a href="./aimerykong_files/2024-2-CISC7401-001-ADVANCED-MACHINE-LEARNING.pdf"  target="_blank">syllabus</a>] (Spring 2025, UMacau)
	</li>
	<li>
		[CISC3027] Visual Perception and Learning in the Open World [<a href="./aimerykong_files/CISC3027-2024-2025_Shu-KONG.pdf"  target="_blank">syllabus</a>] (Fall 2024, UMacau)
	</li>
	<li>
		[CISC7014] Visual Perception and Learning in the Open World [<a href="./aimerykong_files/CISC7014_2023-2024.pdf"  target="_blank">syllabus</a>] (Spring 2024, UMacau)
	</li>
	<li>
		[CISC3027] Visual Perception and Learning in the Open World (Fall 2023, UMacau)
	</li>
	<li>
	[CSCE689] Visual Learning for Visual Recognition (Spring 2023, TAMU)
	</li>
</ul>


		
<h3 style="font-size:20px;font-family:Times">Professional services</h3>
<ul style="font-size:15px;font-family:Times;line-height:18px">	
	<li>	Area Chair at <a href="https://neurips.cc/Conferences/2024"  target="_blank">NeurIPS'24</a>, <a href="https://neurips.cc/"  target="_blank">NeurIPS'25</a>;
	</li>
	<li>	<a href="https://cvpr.thecvf.com/Conferences/2024"  target="_blank">Demo Chair at CVPR'24</a>;
	</li>
	<li>	organizer: <a href="https://vplow.github.io/"  target="_blank">Open-World Vision workshops at CVPR & WACV</a>; 
	</li>
	<li>	<a href="https://www.sciencedirect.com/journal/pattern-recognition"  target="_blank">Associate Editor: Pattern Recognition</a>;
	</li>
	<li>	Panalist: NSF GRFP
	</li>	
	<li>	Reviewers/program committee: CVPR, ICCV, ECCV, ICLR, NeurIPS, ACL, ICML, IJCV, PAMI, Nature, PLOS One, RA-L, TIP, etc.
	</li>	
</ul>







<h3 style="font-size:20px;font-family:Times">Updates</h3>

<ul style="font-size:15px;font-family:Times;line-height:20px">


<li>
<p>
Congratulations to <a href="https://anthropology.sas.upenn.edu/people/jennifer-feng/">Jennifer</a> on the <a href="https://www.cambridge.org/core/journals/paleobiology"  target="_blank">Paleobiology
<img src="https://www.cambridge.org/core/services/aop-file-manager/file/567a62235cefc91d0c2d4094/Paleontological-Society-Logo-680x460px-1-.jpg", height="25"></a> 
	paper "<a href="https://doi.org/10.1101/2025.01.05.631390"  target="_blank">Addressing the Open World: Detecting and Segmenting Pollen on Palynological Slides with Deep Learning</a>"! (5/15/2025)
</a>
</li>

	

<li>
<p>
Congratulations to <a href="https://www.linkedin.com/in/marc-elie-adaime-8b756270/">Marc-Elie</a> on the <a href="https://nph.onlinelibrary.wiley.com/journal/14698137"  target="_blank">New Phytologist 
<img src="https://nph.onlinelibrary.wiley.com/pb-assets/journal-banners/14698137-1527608518210.jpg", height="20"></a> 
	paper "<a href="https://www.biorxiv.org/content/10.1101/2025.01.07.631571v1"  target="_blank">Pollen Morphology, Deep Learning, Phylogenetics, and the Evolution of Environmental Adaptations in Podocarpus</a>"! (5/4/2025)
</a>
</li>


<li>
<p>
Presented at <a href="https://wacv2025.thecvf.com/"  target="_blank">WACV'25</a> Workshop on <a href="https://sites.google.com/ucr.edu/cooolsworkshop/home"  target="_blank">Out-of-Label Hazards in Autonomous Driving</a>, 
	titled "The Concept Misalignment between Experts and AI, from Data Labeling to Data Versioning" [<a href="./aimerykong_files/talk_WACV_COOOL_20250304.pdf"  target="_blank">slides</a>]! (3/4/2025)
</a>
</li>

	

<li>
<p>
Congratulations to  <a href="https://tian1327.github.io/">Tian</a>, <a href="https://openreview.net/profile?id=~Huixin_Zhang1">Huixin</a> and <a href="https://shubhamprshr27.github.io/">Shubham</a> on the <a href="https://cvpr.thecvf.com/"  target="_blank">CVPR'25
<img src="./aimerykong_files/cvpr2025_logo.jpg", height="25"></a> 
	paper "<a href="https://tian1327.github.io/SWAT/"  target="_blank">Few-Shot Recognition via Stage-Wise Augmented Finetuning</a>"! (2/27/2025)
</a>
</li>


<li>
<p>
Congratulations to <a href="https://ics.uci.edu/~yunhaz5/">Yunhan</a>, <a href="https://shenqq377.github.io/">Qianqian</a> and <a href="https://nahyunkwon.github.io/">Nahyun</a> on the <a href="https://cvpr.thecvf.com/"  target="_blank">CVPR'25
<img src="./aimerykong_files/cvpr2025_logo.jpg", height="25"></a> 
	paper "<a href="https://shenqq377.github.io/IDOW/" target="_blank">Solving Instance Detection from an Open-World Perspective</a>"! (2/27/2025)
</a>
</li>


	
<li>
<p>
Congratulations to <a href="http://www.nicholasmilef.com/">Nick Milef</a> on the successful PhD defense 
	and the new position as a Research Scientist at Meta <img src="./aimerykong_files/meta-logo.jpg", height="20"></a>! (2/11/2025)
</a>
</li>



<li>
<p>
Congratulations to <a href="https://www.linkedin.com/in/marc-elie-adaime-8b756270/">Dr. Marc-Élie Adaimé</a> on the successful PhD defense 
	and the new position as a Research Fellow at <a href="https://www.si.edu/">Smithsonian Institution <img src="./aimerykong_files/smithsonian-institution.jpg", height="30"></a>! (1/20/2025)
</a>
</li>


<li>
<p>
Presented at <a href="https://accv2024.org/">ACCV'24</a> Workshop on <a href="https://insdet.github.io/">Object Instance Detection</a>, 
	titled "Instance Detection and Tracking in the Open World" [<a href="./aimerykong_files/presentation_ACCV2024_InstanceDetection.pdf">slides</a>]! (12/9/2024)
</a>
</li>

	
<li>
<p>
Congratulations to <a href="https://www.linkedin.com/in/runzhi-wang">Runzhi</a> on the
	<a href="https://spie.org/advanced-lithography/presentation/Automated-identification-of-repeated-chip-layout-patterns/13425-39"  target="_blank">DTCO and Computational
	Patterning <img src="./aimerykong_files/SPIE_Lithography_logo.jpg", height="30"></a> 
	paper "<a href="https://spie.org/advanced-lithography/presentation/Automated-identification-of-repeated-chip-layout-patterns/13425-39"  target="_blank">Automated Identification of Repeated Chip Layout Patterns</a>",
	which is accepted for <font color=ff3399>oral presentation</font>! (11/26/2024)
</a>
</li>




<li>
<p>
Congratulations to <a href="https://bd.linkedin.com/in/hasnat-md-abdullah">Hasnat</a> on the <a href="https://wacv2025.thecvf.com/"  target="_blank">WACV'25 
<img src="./aimerykong_files/wacv2025-logo.jpg", height="30"></a> 
	paper "<a href="https://arxiv.org/abs/2410.01180"  target="_blank">UAL-Bench: The First Comprehensive Unusual Activity Localization Benchmark</a>"! (10/29/2024)
</a>
</li>



<li>
<p>
Congratulations to <a href="https://anishmadan23.github.io/">Anish</a> and <a href="http://www.neeharperi.com/">Neehar</a> on the <a href="https://nips.cc/Conferences/2024"  target="_blank">NeurIPS'24 
<img src="./aimerykong_files/NeurIPS2022_logo.jpg", height="35"></a> 
	paper "<a href="https://arxiv.org/abs/2312.14494"  target="_blank">Revisiting Few-Shot Object Detection with Vision-Language Models</a>"! (9/26/2024)
</a>
</li>


	
<li>
<p>
Congratulations to <a href="https://sites.google.com/site/samiashafique067/">Samia</a> on the  
	<a href="https://eccv.ecva.net/"  target="_blank">ECCV <img src="./aimerykong_files/eccv2024-logo.jpg", height="25"></a>
	paper "<a href="https://arxiv.org/abs/2404.16972"  target="_blank">CriSp: Leveraging Tread Depth Maps for Enhanced Crime-Scene Shoeprint Matching</a>"! (7/1/2024)
</a>
</li>

	
<li>
<p>
Congratulations to <a href="https://github.com/WangYZ1608">Yuzhu</a> on the  
	<a href="https://eccv.ecva.net/"  target="_blank">ECCV <img src="./aimerykong_files/eccv2024-logo.jpg", height="25"></a>
	paper "<a href="https://github.com/WangYZ1608/Knowledge-Distillation-via-ND"  target="_blank">Improving Knowledge Distillation via Regularizing Feature Norm and Direction</a>", 
	which is accepted for <font color=ff3399>oral presentation</font>! (7/1/2024)
</a>
</li>
	



<li>
<p>
Congratulations to <a href="https://anirudh-chakravarthy.github.io/">Anirudh</a> and <a href="https://g-meghana-reddy.github.io/">Meghana</a> on the  
	<a href="https://link.springer.com/article/10.1007/s11263-024-02166-9"  target="_blank">IJCV <img src="./aimerykong_files/logo_ijcv.jpg", height="25"></a>
	paper "<a href="https://link.springer.com/article/10.1007/s11263-024-02166-9"  target="_blank">Lidar Panoptic Segmentation in an Open World</a>"! (6/4/2024)
</a>
</li>
	
<li>
<p>
Congratulations to <a href="https://www.linkedin.com/in/elvishelvisshi/">Jia</a> on the  
	<a href="https://icml.cc/Conferences/2024"  target="_blank">ICML'24<img src="./aimerykong_files/icml2024-logo.jpg", height="25"></a>
	paper "<a href="https://arxiv.org/abs/2407.16067"  target="_blank">LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies</a>",
	which is accepted for <font color=ff3399>oral presentation</font>! (5/2/2024)
</a>
</li>

	
<li>
<p>
Congratulations to <a href="http://www.nicholasmilef.com/">Nick</a> on the  
	<!--a href="https://s2024.siggraph.orgs/"  target="_blank">sggrph<img src="./aimerykong_files/siggraph24s.jpg", height="25"></a-->
	<a href="https://s2024.siggraph.org/"  target="_blank">SIGGRAPH'24 <img src="./aimerykong_files/siggraph24.jpg", height="25"></a> 
	paper "<a href="https://res.cloudinary.com/hes7dykgv/image/upload/v1716166902/Documents/ennwo812muxljckwcooz.pdf"  target="_blank">Towards Unstructured Unlabeled Optical Mocap: A Video Helps!</a>"! (3/25/2024)
</a>
</li>


	
<li>
<p>
Call for demos at <a href="https://cvpr.thecvf.com/Conferences/2024"  target="_blank"><img src="./aimerykong_files/cvpr24-logo2.jpg", height="20">CVPR'24</a>! If you have exciting demos, 
	consider to submit and present at CVPR in June at Seattle! 
	The submission is trivial and its deadline is this Sunday, Mar 10, 2024 11:59 PM AOE. 
	<a href='https://cvpr.thecvf.com/Conferences/2024/CallForDemos'>Here is the link to submit</a>.
</li>


	
<li>
<p>
Congratulations to <a href="https://shubhamprshr27.github.io/">Shubham</a>, <a href="https://tian1327.github.io/">Tian</a>, 
	and <a href="https://linzhiqiu.github.io/">Zhiqiu</a> on the 
	<a href="https://cvpr.thecvf.com/Conferences/2024"  target="_blank">CVPR'24
<img src="./aimerykong_files/cvpr24-logo2.jpg", height="20"></a> 
	paper "<a href="https://shubhamprshr27.github.io/neglected-tails-of-vlms"  target="_blank">The Neglected Tails of Vision-Language Models</a>"! (2/27/2024)
</a>
</li>


<li>
<p>
Congratulations to <a href="https://ics.uci.edu/~yunhaz5/">Yunhan</a> on the 
	<a href="https://cvpr.thecvf.com/Conferences/2024"  target="_blank">CVPR'24
<img src="./aimerykong_files/cvpr24-logo2.jpg", height="20"></a> 
	paper "<a href="https://arxiv.org/abs/2312.04117"  target="_blank">Instance Tracking in 3D Scenes from Egocentric Videos</a>"! (2/27/2024)
</a>
</li>


<li>
<p>
Congratulations to <a href="https://github.com/SunzeY">Zeyi</a> and <a href="https://github.com/Aleafy">Ye</a> on the 
	<a href="https://cvpr.thecvf.com/Conferences/2024"  target="_blank">CVPR'24
<img src="./aimerykong_files/cvpr24-logo2.jpg", height="20"></a> 
	paper "<a href="https://aleafy.github.io/alpha-clip/"  target="_blank">Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</a>"! (2/27/2024)
</a>
</li>



<li>
<p>
Congratulations to <a href="http://xuxiaogang.com/">Xiaogang</a> on the 
	<a href="https://cvpr.thecvf.com/Conferences/2024"  target="_blank">CVPR'24
<img src="./aimerykong_files/cvpr24-logo2.jpg", height="20"></a> 
	paper "<a href="https://arxiv.org/pdf/2403.06793.pdf"  target="_blank">Boosting Image Restoration via Priors from Pre-trained Models</a>"! (2/27/2024)
</a>
</li>
	

	

<li>
<p>
Congratulations to <a href="https://nahyunkwon.github.io/">Nahyun</a>, <a href="https://hcied.info/hcieders.html">Emory, Muhammad, and Joanne</a> on the 
	<a href="https://chi2024.acm.org/"  target="_blank">CHI'24
<img src="./aimerykong_files/CHI2024.jpg", height="25"></a> 
	paper "<a href="https://arxiv.org/abs/2401.15996"  target="_blank">AccessLens: Auto-detecting Inaccessibility of Everyday Objects</a>"! (1/19/2024)
</a>
</li>

<li>
<p>
Congratulations to <a href="https://scholar.google.com/citations?user=qYMFo1wAAAAJ&hl=en">Marc-Elie</a> on the <a href="https://academic.oup.com/pnasnexus"  target="_blank">PNAS Nexus 
<img src="./aimerykong_files/pnas-nexus-logo.png", height="20"></a> 
	paper "<a href="https://www.biorxiv.org/content/10.1101/2023.07.09.545296v2"  target="_blank">Deep Learning Approaches to the Phylogenetic Placement of Extinct Pollen Morphotypes</a>"! (11/21/2023)
</a>
</li>

	

<li>
<p>
Congratulations to <a href="https://www.linkedin.com/in/shubhamprshr">Shubham</a> on the <a href="https://2023.emnlp.org/"  target="_blank">EMNLP'23 
<img src="./aimerykong_files/emnlp2023-logo.png", height="20"></a> 
	paper "<a href="http://arxiv.org/abs/2310.09929"  target="_blank">Prompting Scientific Names for Zero-Shot Species Recognition</a>"! (10/17/2023)
</a>
</li>


<li>
<p>
Congratulations to <a href="https://ics.uci.edu/~yunhaz5/">Yunhan</a>, <a href="https://shenqq377.github.io/">Qianqian</a> and <a href="https://nahyunkwon.github.io/">Nahyun</a> on the <a href="https://nips.cc/Conferences/2023"  target="_blank">NeurIPS'23 
<img src="./aimerykong_files/NeurIPS2022_logo.jpg", height="35"></a> 
	paper "<a href="https://github.com/insdet/instance-detection"  target="_blank">A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture</a>"! (10/17/2023)
</a>
</li>


	
<li>
<p>
Congratulations to <a href="https://scholar.google.com.hk/citations?user=Wx8ChLcAAAAJ&hl=zh-CN">Meng</a> on the <a href="https://nips.cc/Conferences/2023"  target="_blank">NeurIPS'23 
<img src="./aimerykong_files/NeurIPS2022_logo.jpg", height="35"></a> 
	paper "<a href="https://github.com/OpenRobotLab/OV_PARTS"  target="_blank">OV-PARTS: Towards Open-Vocabulary Part Segmentation</a>"! (10/17/2023)
</a>
</li>



	
	
	


<li>
<p>
Our <a href="https://vplow.github.io/vplow_3rd.html"  target="_blank">"3rd Workshop of Visual Perception and Learning in an Open World" <img src="http://vplow.github.io/open-world-vision_files/logo.png", height="23"></a>
will be held in conjunction with <a href="https://cvpr2023.thecvf.com/"  target="_blank">CVPR'23 on June 18, 2023!
<img src="./aimerykong_files/cvpr_banner_homepage.svg", height="23">
</a>   (6/17/2023)
</a>
</li>	
	

<li>
<p>
Congratulations to <a href="https://sites.google.com/site/samiashafique067/">Samia</a> on the <a href="https://wacv2023.thecvf.com/home"  target="_blank">WACV'23 
<img src="./aimerykong_files/WACV2023_logo.jpg", height="20"></a> 
	paper "<a href="https://github.com/Samia067/ShoeRinsics"  target="_blank">Creating a Forensic Database of Shoeprints from Online Shoe Tread Photos</a>"! (10/10/2022)
</a>
</li>



<li>
<p>
Congratulations to <a href="https://www.linkedin.com/in/shubham-gupta94/">Shubham</a> and <a href="https://www.linkedin.com/in/jeet-kanjani-a86062107/">Jeet</a> on the <a href="https://wacv2023.thecvf.com/home"  target="_blank">WACV'23 
<img src="./aimerykong_files/WACV2023_logo.jpg", height="20"></a> 
	paper "<a href="https://arxiv.org/abs/2211.13858"  target="_blank">Far3Det: Towards Far-Field 3D Detection</a>"! (10/10/2022)
</a>
</li>


<li>
<p>
Congratulations to <a href="https://linzhiqiu.github.io/">Zhiqiu</a> on the <a href="https://nips.cc/Conferences/2022"  target="_blank">NeurIPS'22 
<img src="./aimerykong_files/NeurIPS2022_logo.jpg", height="40"></a> 
	paper "<a href="https://arxiv.org/abs/2210.04993"  target="_blank">Continual Learning With an Evolving Class Ontology</a>"! Read more <a href="https://linzhiqiu.github.io/papers/leco/">in the webstie</a>. (9/14/2022)
</a>
</li>
	
	
<li>
<p>
Congratulations to <a href="http://www.neeharperi.com/">Neehar</a> on the <a href="https://corl2022.org"  target="_blank">CoRL'22 <img src="./aimerykong_files/CoRL2022_logo.jpg", height="30"></a> 

	paper "<a href="https://arxiv.org/abs/2211.08691"  target="_blank">Towards Long Tailed 3D Detection</a>"! (9/10/2022)
</a>
</li>	

		
<li>
<p>
Our 2nd workshop of <a href="https://www.cs.umd.edu/~pulkit/DNOW_workshop_2/" target="_blank">Dealing with the Novelty in Open Worlds</a> 
will be held on Jan 3rd, 2023, at 
<a href="https://wacv2023.thecvf.com/home" target="_blank">WACV'23 <img src="./aimerykong_files/WACV2023_logo.jpg", height="20"></a> in Waikoloa, Hawaii!
(8/15/2021)
</p>
</li>		
		
<li>
<p>
Congratulations to <a href="https://www.linkedin.com/in/yi-ting-chen-b6984694/">Yi-Ting</a>, 
	<a href="https://www.linkedin.com/in/jinghao-shi-82371919b/">Jinghao</a>, and 
	<a href="https://www.linkedin.com/in/zelinye/">Zelin</a> on the <a href="https://eccv2022.ecva.net"  target="_blank">ECCV'22 
<img src="./aimerykong_files/ECCV2022logo.png", height="25"></a> 	
	paper "<a href="https://arxiv.org/abs/2104.02904"  target="_blank">Multimodal Object Detection via Probabilistic Ensembling</a>",
	which is accepted for <font color=ff3399>oral presentation</font>!
	The proposed method was ranked the first in the <a href="https://eval.ai/web/challenges/challenge-page/1247/leaderboard/3137">KAIST challenge leaderboard</a>.
	Code is available in <a href="https://github.com/Jamie725/RGBT-detection">github</a>. (7/8/2022)
</a>
</li>
	

<li>
<p>
Our paper "<a href="https://github.com/aimerykong/aimerykong.github.io/raw/main/OpenGAN_files/PAMI_OpenGAN_accepted_version.pdf"  target="_blank">OpenGAN: Open-Set Recognition Via Open Data Generation</a>" 
	has been published by <a href="https://ieeexplore.ieee.org/document/9799769"  target="_blank">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</a>  (6/18/2022)
</li>
		

<li>
<p>
Our paper "<a href="https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13917"  target="_blank">Automated identification of diverse Neotropical pollen samples using convolutional neural networks</a>" has been published by <a href="https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13917"  target="_blank">Methods in Ecology and Evolution</a>  (6/12/2022)
</a>
</li>



<li>
<p>
Our workshop <a href="http://vplow.github.io/vplow.html"  target="_blank">"Visual Perception and Learning in an Open World" <img src="http://vplow.github.io/open-world-vision_files/logo.png", height="23"></a>
will be held in conjunction with <a href="https://cvpr2022.thecvf.com/"  target="_blank">CVPR'22
<img src="./aimerykong_files/CVPR2022logo.png", height="23">
</a>   (4/29/2022)
</a>
</li>


<li>
<p>
Congratulations to <a href="https://www.linkedin.com/in/salshamm">Shaden</a> on the <a href="https://cvpr2022.thecvf.com/"  target="_blank">CVPR'22 <img src="./aimerykong_files/CVPR2022logo.png", height="23"></a>
paper "<a href="https://arxiv.org/abs/2203.14197">Long-Tailed Recognition via Weight Balancing</a>"!
Code is available in the <a href="https://github.com/ShadeAlsha/LTR-weight-balancing">github page</a>!
(3/2/2022)
</li>




<li>
<p>
Our paper <a href="https://aimerykong.github.io/OpenGAN.html">"OpenGAN: Open-Set Recognition via Open Data Generation"</a>
received honorable mention for <font color=ff3399>Best Paper / Marr Prize</font> at 
<a href="http://iccv2021.thecvf.com/" target="_blank">ICCV'21 <img src="./aimerykong_files/iccv21_logo.png", height="23"></a>.
Watch this <a href="https://youtu.be/CNYqYXyUHn0">12min video</a>
(10/12/2021) </p>
</li>


<li>
<p>
Our in-person workshop <a href="https://www.cs.umd.edu/~pulkit/DNOW_workshop/" target="_blank">Dealing with the Novelty in Open Worlds</a> 
will be held on Jan 4, 2022, in conjunction with 
<a href="http://wacv2022.thecvf.com/home" target="_blank">WACV'22 <img src="./aimerykong_files/logo_wacv2022.jpg", height="23"></a>  
(8/27/2021)
</p>
</li>


<li>
<p>
Our challenge <a href="https://eval.ai/web/challenges/challenge-page/1041/overview" target="_blank">Open-World Image Classification <img src="http://vplow.github.io/open-world-vision_files/logo.png", height="23"></a> is online now! The challenge will be held in conjunction with
our <a href="http://vplow.github.io/open-world-vision.html">Open World Vision workshop</a> and <a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR'21 <img src="./aimerykong_files/logo_cvpr2021.png", height="23"></a>
(5/14/2021)
</p>
</li>


<li>
<p>
	Congratulations to <a href="https://www.ics.uci.edu/~yunhaz5" target="_blank">Yunhan Zhao</a> on the paper <a href="https://www.ics.uci.edu/~yunhaz5/cvpr2021/cpp.html">
	"Camera Pose Matters: Improving Depth Prediction by Mitigating Pose Distribution Bias"</a>, which is accepted for <font color=ff3399>oral presentation</font> by 
<a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR'21 <img src="./aimerykong_files/logo_cvpr2021.png", height="23"></a> (03/3/2021) </p>
</li>


<li>
<p>
Our virtual workshop <a href="http://vplow.github.io/open-world-vision.html" target="_blank">Open-World Vision
	<img src="http://vplow.github.io/open-world-vision_files/logo.png", height="23"></a> will be held in conjunction with 
<a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR'21 <img src="./aimerykong_files/logo_cvpr2021.png", height="23"></a>  
(12/11/2020)
</p>
</li>



<li>
<p>
Our work is published on <a href="https://doi.org/10.1073/pnas.2007324117" target="_blank">
<img src="./aimerykong_files/pnas_logo.jpg" height="20"></a>
"<a href="https://doi.org/10.1073/pnas.2007324117" target="_blank">Improving the Taxonomy of Fossil Pollen using Convolutional Neural Networks and Superresolution Microscopy</a>",
and featured by the <a href="https://www.nsf.gov/discoveries/disc_summ.jsp?cntn_id=301568&org=NSF&from=news" target="_blank">NSF<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/NSF_logo.png/220px-NSF_logo.png", height="23"></a></a>.
(09/14/2020)
</p>
</li>

</ul>


</div>

<br>
<br>
<br>
<br>
<br>
<br>


</div>




</body>
</html>
