<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Open+Sans" />
	<link rel="stylesheet" type="text/css" href='static/css/bootstrap.min.css' />
	<link rel="stylesheet" type="text/css" href='static/css/styles.css'/>
	<script src=''></script>
	<title>Shu Kong</title>
	<link rel="icon" href="aimerykong_files/profile2.png" type="img/jpg">
</head>

	
	
<body>
	<div class="my-container">
		<h1> Shu Kong </h1>
		<span> Assistant Professor; University of Macau, Texas A&M University <a href="https://github.com/aimerykong">[GitHub]</a>  <a href="http://scholar.google.com/citations?user=sm9FdLoAAAAJ&hl=en">[Google Scholar]</a> </span>

		<div class="btn-group btn-group-justified" role="group" aria-label="Justified button group">
			<a href="index.html" class="btn btn-default" role="button">Home</a>
			<a href="publication.html" class="btn btn-default" role="button">Publication</a>
			<a href="group.html" class="btn btn-default" role="button">Group</a>
			<a href="contact.html" class="btn btn-default" role="button">Contact</a>
    	</div>

    	<div class="content">
    		<div class="row">
				<div class="col-sm-2">
					<img class="img-responsive img-rounded" src="./aimerykong_files/profile_shu.jpg">
				</div>
				<div class="col-sm-10">
        <p style="font-size:16px;font-family:Times"> I am on the faculty of <a href="https://www.fst.um.edu.mo/">FST</a>,  <a href="https://www.um.edu.mo/">University of Macau</a>, and <a href="https://engineering.tamu.edu/cse/index.html">CSE</a>,  <a href="https://www.tamu.edu/">Texas A&M University</a>.
		  I lead the <a href="group.html">Computer Vision Lab</a>. 
		  I did my postdoc training at the <a href="https://www.ri.cmu.edu/">Robotics Institute</a>, <a href="https://www.cmu.edu/">CMU</a>, 
		  working with <a href="http://www.cs.cmu.edu/~deva/">Deva Ramanan</a>. 
		  I received my PhD from <a href="http://www.uci.edu/">UC-Irvine</a>, where I worked with <a href="http://www.ics.uci.edu/~fowlkes/">Charless Fowlkes</a>.
	</p>
	
	<p style="font-size:16px;font-family:Times">
		My research lies in <strong>Computer Vision</strong>, and its interactions with other fields (e.g., machine learning, robotics, NLP, HCI, and graphics), broad
		applications (e.g., AR/VR, autonomous driving, etc.), and diverse disciplines (e.g., biology, paleoecology, psychology, special education, etc.). 
		My current research focus is on <a href="https://vplow.github.io"><strong>Visual Perception via Learning in the Open World (VPLOW)</strong></a>. 
		My recent paper on this topic was recognized for <a href="https://arxiv.org/abs/2104.02939">Best Paper / Marr Prize at ICCV 2021</a>.
		I also actively apply my algorithms to interdisciplinary research including building a high-throughput pollen analysis system,
		which was featured by the National Science Foundation as that “<a href="https://beta.nsf.gov/news/modern-computational-tools-open-new-era-fossil-pollen-research">opens a new era of fossil pollen research</a>”.
	</p>


					
	<!--p style="font-size:16px;font-family:Times">
		My research interests span <strong>Computer Vision</strong> and <strong>applied Machine Learning</strong> (CV/ML), 
		and their applications to autonomous vehicles and research in natural science. 
		My current research in CV/ML focuses on <a href="https://vplow.github.io"><strong>Visual Perception via Learning in the Open World (VPLOW)</strong></a>. 
		My recent paper on this topic was recognized for <a href="https://arxiv.org/abs/2104.02939">Best Paper / Marr Prize at ICCV 2021</a>.
		I also actively apply my algorithms to interdisciplinary research including building a high-throughput pollen analysis system,
		which was featured by the National Science Foundation as that “<a href="https://beta.nsf.gov/news/modern-computational-tools-open-new-era-fossil-pollen-research">opens a new era of fossil pollen research</a>”.
				</p-->
					
	<p style="font-size:16px;font-family:Times">
		<font color="#8B0000">
			I am actively looking for self-motivated PhD students at <a href="https://www.fst.um.edu.mo/">CIS, FST, University of Macau</a>.
			PhD students will be fully funded with fellowships.
		</font>
			I am NOT actively looking for PhD students at <a href="https://engineering.tamu.edu/cse/admissions-and-aid/graduate-admissions.html">CSE, TAMU</a>.
		
	</p>


<h3 style="font-size:20px;font-family:Times">contact</h3>
<ul style="font-size:15px;font-family:Times;line-height:15px">
<li>email: shu [at] tamu [dot] edu   [issues related to TAMU]</li>
<li>email: skong [at] um [dot] edu [dot] mo   [issues related to the University of Macau]</li>
<li>gmail: aimerykong [at] gmail [dot] com    [issues related to others; unlikely to reply]</li>
</ul>
</div>
</div>



		
<h3 style="font-size:20px;font-family:Times">Links</h3>
<ul style="font-size:13px;font-family:Times;line-height:15px">	
	<li>
	<a href="https://vplow.github.io/vplow_4th.html">The 4th Open World Vision Workshop: Visual Perception via Learning in an Open World (VPLOW) at CVPR'24</a>
	</li>
	<li>
	<a href="https://vplow.github.io/vplow_3rd.html">The 3rd Open World Vision Workshop: Visual Perception via Learning in an Open World (VPLOW) at CVPR'23</a>
	</li>
	<li>
	<a href="https://www.cs.umd.edu/~pulkit/DNOW_workshop_2/">The 2nd Workshop on Dealing with Novelty in Open Worlds (DNOW) at WACV'23</a>
	</li>
	<li>
	<a href="http://vplow.github.io/vplow.html">The 2nd Open World Vision Workshop: Visual Perception and Learning in an Open World (VPLOW) at CVPR'22</a>
	</li>
	<li>
	<a href="https://www.cs.umd.edu/~pulkit/DNOW_workshop/">The 1st Workshop on Dealing with Novelty in Open Worlds (DNOW) at WACV'22</a>
	</li>
	<li>
	<a href="http://vplow.github.io/open-world-vision.html">The 1st Open World Vision Workshop at CVPR'21</a>
	</li>
</ul>

		
<h3 style="font-size:20px;font-family:Times">Teaching</h3>
<ul style="font-size:13px;font-family:Times;line-height:15px">
	<li>
		[CISC7014] Advanced Topics in Computer Science: Visual Perception and Learning in the Open World [<a href="./aimerykong_files/CISC7014_2023-2024.pdf
">syllabus</a>] (Spring 2024, UMacau)
	</li>
	<li>
		[CISC3027] Special Topics in Computer and Information Science: Visual Perception and Learning in the Open World (Fall 2023, UMacau)
	</li>
	<li>
	[CSCE689] Visual Learning for Visual Recognition (Spring 2023, TAMU)
	</li>
</ul>


		
<h3 style="font-size:20px;font-family:Times">Professional activities</h3>
<ul style="font-size:13px;font-family:Times;line-height:15px">	
	<li>
		Demo Chair: <a href="https://cvpr.thecvf.com/Conferences/2024">CVPR'24</a>		
	</li>
	<li>
		Workshop Chair: <a href="https://vplow.github.io/">Open-World Vision workshops at CVPR, WACV</a>
	</li>
	<li>
		Panalist: NSF GRFP
	</li>	
	<li>
		Reviewers/program committee: CVPR, ICCV, ECCV, ICLR, NeurIPS, ICML, IJCV, PAMI, RA-L, TIP, Nature, PLOS One, etc.
	</li>	
</ul>







<h3 style="font-size:20px;font-family:Times">Research Updates</h3>

<ul style="font-size:13px;font-family:Times">








<li>
<p>
Congratulations to <a href="http://www.nicholasmilef.com/">Nick</a> on the SIGGRAPH'24
<img src="./aimerykong_files/siggraph24.jpg", height="20"></a> 
	paper "<a href="./"  target="_blank">Towards Unstructured Unlabeled Optical Mocap: A Video Helps!</a>"! (3/23/2024)
</a>
</li>


	
<li>
<p>
Call for demos at <img src="./aimerykong_files/cvpr24-logo2.jpg", height="20">CVPR'24</a>! If you have exciting demos, 
	consider to submit and present at CVPR in June at Seattle! 
	The submission is trivial and its deadline is this Sunday, Mar 10, 2024 11:59 PM AOE. 
	<a href='https://cvpr.thecvf.com/Conferences/2024/CallForDemos'>Here is the link to submit</a>.
</li>


	
<li>
<p>
Congratulations to <a href="https://shubhamprshr27.github.io/">Shubham</a>, <a href="https://www.linkedin.com/in/tian1327/">Tian</a>, 
	and <a href="https://linzhiqiu.github.io/">Zhiqiu</a> on the CVPR'24
<img src="./aimerykong_files/cvpr24-logo2.jpg", height="20"></a> 
	paper "<a href="https://shubhamprshr27.github.io/neglected-tails-of-vlms"  target="_blank">The Neglected Tails of Vision-Language Models</a>"! (2/27/2024)
</a>
</li>


<li>
<p>
Congratulations to <a href="https://ics.uci.edu/~yunhaz5/">Yunhan</a> on the CVPR'24
<img src="./aimerykong_files/cvpr24-logo2.jpg", height="20"></a> 
	paper "<a href="https://arxiv.org/abs/2312.04117"  target="_blank">Instance Tracking in 3D Scenes from Egocentric Videos</a>"! (2/27/2024)
</a>
</li>


<li>
<p>
Congratulations to <a href="https://github.com/SunzeY">Zeyi</a> and <a href="https://github.com/Aleafy">Ye</a> on the CVPR'24
<img src="./aimerykong_files/cvpr24-logo2.jpg", height="20"></a> 
	paper "<a href="https://aleafy.github.io/alpha-clip/"  target="_blank">Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</a>"! (2/27/2024)
</a>
</li>



<li>
<p>
Congratulations to <a href="http://xuxiaogang.com/">Xiaogang</a> on the CVPR'24
<img src="./aimerykong_files/cvpr24-logo2.jpg", height="20"></a> 
	paper "<a href="https://arxiv.org/pdf/2403.06793.pdf"  target="_blank">Boosting Image Restoration via Priors from Pre-trained Models</a>"! (2/27/2024)
</a>
</li>
	

	

<li>
<p>
Congratulations to <a href="https://nahyunkwon.github.io/">Nahyun</a>, <a href="https://hcied.info/hcieders.html">Emory, Muhammad, and Joanne</a> on the CHI'24
<img src="./aimerykong_files/CHI2024.jpg", height="20"></a> 
	paper "<a href="https://arxiv.org/abs/2401.15996"  target="_blank">AccessLens: Auto-detecting Inaccessibility of Everyday Objects</a>"! (1/19/2024)
</a>
</li>

<li>
<p>
Congratulations to <a href="https://scholar.google.com/citations?user=qYMFo1wAAAAJ&hl=en">Marc-Elie</a> on the <a href="https://academic.oup.com/pnasnexus"  target="_blank">PNAS Nexus 
<img src="./aimerykong_files/pnas-nexus-logo.png", height="20"></a> 
	paper "<a href="https://www.biorxiv.org/content/10.1101/2023.07.09.545296v2"  target="_blank">Deep Learning Approaches to the Phylogenetic Placement of Extinct Pollen Morphotypes</a>"! (11/21/2023)
</a>
</li>

	

<li>
<p>
Congratulations to <a href="https://www.linkedin.com/in/shubhamprshr">Shubham</a> on the <a href="https://2023.emnlp.org/"  target="_blank">EMNLP'23 
<img src="./aimerykong_files/emnlp2023-logo.png", height="20"></a> 
	paper "<a href="http://arxiv.org/abs/2310.09929"  target="_blank">Prompting Scientific Names for Zero-Shot Species Recognition</a>"! (10/17/2023)
</a>
</li>


<li>
<p>
Congratulations to <a href="https://ics.uci.edu/~yunhaz5/">Yunhan</a>, Qianqian and <a href="https://nahyunkwon.github.io/">Nahyun</a> on the <a href="https://nips.cc/Conferences/2023"  target="_blank">NeurIPS'23 
<img src="./aimerykong_files/NeurIPS2022_logo.jpg", height="35"></a> 
	paper "<a href="https://github.com/insdet/instance-detection"  target="_blank">A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture</a>"! (10/17/2023)
</a>
</li>


	
<li>
<p>
Congratulations to <a href="https://scholar.google.com.hk/citations?user=Wx8ChLcAAAAJ&hl=zh-CN">Meng</a> on the <a href="https://nips.cc/Conferences/2023"  target="_blank">NeurIPS'23 
<img src="./aimerykong_files/NeurIPS2022_logo.jpg", height="35"></a> 
	paper "<a href="https://github.com/OpenRobotLab/OV_PARTS"  target="_blank">OV-PARTS: Towards Open-Vocabulary Part Segmentation</a>"! (10/17/2023)
</a>
</li>



	
	
	


<li>
<p>
Our <a href="https://vplow.github.io/vplow_3rd.html"  target="_blank">"3rd Workshop of Visual Perception and Learning in an Open World" <img src="http://vplow.github.io/open-world-vision_files/logo.png", height="23"></a>
will be held in conjunction with <a href="https://cvpr2023.thecvf.com/"  target="_blank">CVPR'23 on June 18, 2023!
<img src="./aimerykong_files/cvpr_banner_homepage.svg", height="23">
</a>   (6/17/2023)
</a>
</li>	
	

<li>
<p>
Congratulations to <a href="https://sites.google.com/site/samiashafique067/">Samia</a> on the <a href="https://wacv2023.thecvf.com/home"  target="_blank">WACV'23 
<img src="./aimerykong_files/WACV2023_logo.jpg", height="20"></a> 
	paper "<a href="https://github.com/Samia067/ShoeRinsics"  target="_blank">Creating a Forensic Database of Shoeprints from Online Shoe Tread Photos</a>"! (10/10/2022)
</a>
</li>



<li>
<p>
Congratulations to <a href="https://www.linkedin.com/in/shubham-gupta94/">Shubham</a> and <a href="https://www.linkedin.com/in/jeet-kanjani-a86062107/">Jeet</a> on the <a href="https://wacv2023.thecvf.com/home"  target="_blank">WACV'23 
<img src="./aimerykong_files/WACV2023_logo.jpg", height="20"></a> 
	paper "<a href="https://arxiv.org/abs/2211.13858"  target="_blank">Far3Det: Towards Far-Field 3D Detection</a>"! (10/10/2022)
</a>
</li>


<li>
<p>
Congratulations to <a href="https://linzhiqiu.github.io/">Zhiqiu</a> on the <a href="https://nips.cc/Conferences/2022"  target="_blank">NeurIPS'22 
<img src="./aimerykong_files/NeurIPS2022_logo.jpg", height="40"></a> 
	paper "<a href="https://arxiv.org/abs/2210.04993"  target="_blank">Continual Learning With an Evolving Class Ontology</a>"! Read more <a href="https://linzhiqiu.github.io/papers/leco/">in the webstie</a>. (9/14/2022)
</a>
</li>
	
	
<li>
<p>
Congratulations to <a href="http://www.neeharperi.com/">Neehar</a> on the <a href="https://corl2022.org"  target="_blank">CoRL'22 <img src="./aimerykong_files/CoRL2022_logo.jpg", height="30"></a> 

	paper "<a href="https://arxiv.org/abs/2211.08691"  target="_blank">Towards Long Tailed 3D Detection</a>"! (9/10/2022)
</a>
</li>	

		
<li>
<p>
Our 2nd workshop of <a href="https://www.cs.umd.edu/~pulkit/DNOW_workshop_2/" target="_blank">Dealing with the Novelty in Open Worlds</a> 
will be held on Jan 3rd, 2023, at 
<a href="https://wacv2023.thecvf.com/home" target="_blank">WACV'23 <img src="./aimerykong_files/WACV2023_logo.jpg", height="20"></a> in Waikoloa, Hawaii!
(8/15/2021)
</p>
</li>		
		
<li>
<p>
Congratulations to <a href="https://www.linkedin.com/in/yi-ting-chen-b6984694/">Yi-Ting</a>, 
	<a href="https://www.linkedin.com/in/jinghao-shi-82371919b/">Jinghao</a>, and 
	<a href="https://www.linkedin.com/in/zelinye/">Zelin</a> on the <a href="https://eccv2022.ecva.net"  target="_blank">ECCV'22 
<img src="./aimerykong_files/ECCV2022logo.png", height="25"></a> 	
	paper "<a href="https://arxiv.org/abs/2104.02904"  target="_blank">Multimodal Object Detection via Probabilistic Ensembling</a>",
	which is accepted for <font color=ff3399>oral presentation</font>!
	The proposed method was ranked the first in the <a href="https://eval.ai/web/challenges/challenge-page/1247/leaderboard/3137">KAIST challenge leaderboard</a>.
	Code is available in <a href="https://github.com/Jamie725/RGBT-detection">github</a>. (7/8/2022)
</a>
</li>
	

<li>
<p>
Our paper "<a href="https://github.com/aimerykong/aimerykong.github.io/raw/main/OpenGAN_files/PAMI_OpenGAN_accepted_version.pdf"  target="_blank">OpenGAN: Open-Set Recognition Via Open Data Generation</a>" 
	has been published by <a href="https://ieeexplore.ieee.org/document/9799769"  target="_blank">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</a>  (6/18/2022)
</li>
		

<li>
<p>
Our paper "<a href="https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13917"  target="_blank">Automated identification of diverse Neotropical pollen samples using convolutional neural networks</a>" has been published by <a href="https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13917"  target="_blank">Methods in Ecology and Evolution</a>  (6/12/2022)
</a>
</li>



<li>
<p>
Our workshop <a href="http://vplow.github.io/vplow.html"  target="_blank">"Visual Perception and Learning in an Open World" <img src="http://vplow.github.io/open-world-vision_files/logo.png", height="23"></a>
will be held in conjunction with <a href="https://cvpr2022.thecvf.com/"  target="_blank">CVPR'22
<img src="./aimerykong_files/CVPR2022logo.png", height="23">
</a>   (4/29/2022)
</a>
</li>


<li>
<p>
Congratulations to <a href="https://www.linkedin.com/in/salshamm">Shaden</a> on the <a href="https://cvpr2022.thecvf.com/"  target="_blank">CVPR'22 <img src="./aimerykong_files/CVPR2022logo.png", height="23"></a>
paper "<a href="https://arxiv.org/abs/2203.14197">Long-Tailed Recognition via Weight Balancing</a>"!
Code is available in the <a href="https://github.com/ShadeAlsha/LTR-weight-balancing">github page</a>!
(3/2/2022)
</li>




<li>
<p>
Our paper <a href="https://aimerykong.github.io/OpenGAN.html">"OpenGAN: Open-Set Recognition via Open Data Generation"</a>
received honorable mention for <font color=ff3399>Best Paper / Marr Prize</font> at 
<a href="http://iccv2021.thecvf.com/" target="_blank">ICCV'21 <img src="./aimerykong_files/iccv21_logo.png", height="23"></a>.
Watch this <a href="https://youtu.be/CNYqYXyUHn0">12min video</a>
(10/12/2021) </p>
</li>


<li>
<p>
Our in-person workshop <a href="https://www.cs.umd.edu/~pulkit/DNOW_workshop/" target="_blank">Dealing with the Novelty in Open Worlds</a> 
will be held on Jan 4, 2022, in conjunction with 
<a href="http://wacv2022.thecvf.com/home" target="_blank">WACV'22 <img src="./aimerykong_files/logo_wacv2022.jpg", height="23"></a>  
(8/27/2021)
</p>
</li>


<li>
<p>
Our challenge <a href="https://eval.ai/web/challenges/challenge-page/1041/overview" target="_blank">Open-World Image Classification <img src="http://vplow.github.io/open-world-vision_files/logo.png", height="23"></a> is online now! The challenge will be held in conjunction with
our <a href="http://vplow.github.io/open-world-vision.html">Open World Vision workshop</a> and <a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR'21 <img src="./aimerykong_files/logo_cvpr2021.png", height="23"></a>
(5/14/2021)
</p>
</li>


<li>
<p>
	Congratulations to <a href="https://www.ics.uci.edu/~yunhaz5" target="_blank">Yunhan Zhao</a> on the paper <a href="https://www.ics.uci.edu/~yunhaz5/cvpr2021/cpp.html">
	"Camera Pose Matters: Improving Depth Prediction by Mitigating Pose Distribution Bias"</a>, which is accepted for <font color=ff3399>oral presentation</font> by 
<a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR'21 <img src="./aimerykong_files/logo_cvpr2021.png", height="23"></a> (03/3/2021) </p>
</li>


<li>
<p>
Our virtual workshop <a href="http://vplow.github.io/open-world-vision.html" target="_blank">Open-World Vision
	<img src="http://vplow.github.io/open-world-vision_files/logo.png", height="23"></a> will be held in conjunction with 
<a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR'21 <img src="./aimerykong_files/logo_cvpr2021.png", height="23"></a>  
(12/11/2020)
</p>
</li>



<li>
<p>
Our work is published on <a href="https://doi.org/10.1073/pnas.2007324117" target="_blank">
<img src="./aimerykong_files/pnas_logo.jpg" height="20"></a>
"<a href="https://doi.org/10.1073/pnas.2007324117" target="_blank">Improving the Taxonomy of Fossil Pollen using Convolutional Neural Networks and Superresolution Microscopy</a>",
and featured by the <a href="https://www.nsf.gov/discoveries/disc_summ.jsp?cntn_id=301568&org=NSF&from=news" target="_blank">NSF<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/NSF_logo.png/220px-NSF_logo.png", height="23"></a></a>.
(09/14/2020)
</p>
</li>

</ul>


</div>

<br>
<br>
<br>
<br>
<br>
<br>


</div>




</body>
</html>
