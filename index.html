<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Open+Sans" />
	<link rel="stylesheet" type="text/css" href='static/css/bootstrap.min.css' />
	<link rel="stylesheet" type="text/css" href='static/css/styles.css'/>
	<script src=''></script>
	<title>Shu Kong</title>
	<link rel="icon" href="aimerykong_files/profile2.png" type="img/jpg">
</head>

	
	
<body>
	<div class="my-container">
		<h1> Shu Kong </h1>
		<span> Assistant Professor, Texas A&M University <a href="https://github.com/aimerykong">[GitHub]</a>  <a href="http://scholar.google.com/citations?user=sm9FdLoAAAAJ&hl=en">[Google Scholar]</a> </span>

		<div class="btn-group btn-group-justified" role="group" aria-label="Justified button group">
			<a href="index.html" class="btn btn-default" role="button">Home</a>
			<a href="publication.html" class="btn btn-default" role="button">Publication</a>
			<a href="group.html" class="btn btn-default" role="button">Group</a>
			<a href="contact.html" class="btn btn-default" role="button">Contact</a>
    	</div>

    	<div class="content">
    		<div class="row">
				<div class="col-sm-2">
					<img class="img-responsive img-rounded" src="./aimerykong_files/profile_shu.jpg">
				</div>
				<div class="col-sm-10">
        <p> I am an incoming assistant professor at <a href="https://www.tamu.edu/"> Texas A&M University</a> in the <a href="https://engineering.tamu.edu/cse/index.html">CSE department</a>, 
		  where I lead the <a href="group.html">Computer Vision Lab</a>. 
		  I am a postdoctoral fellow at the <a href="https://www.ri.cmu.edu/">Robotics Institute</a>, <a href="https://www.cmu.edu/">CMU</a>, 
		  working with <a href="http://www.cs.cmu.edu/~deva/">Deva Ramanan</a>. 
		  I received a Ph.D. from <a href="http://www.uci.edu/">UC-Irvine</a>, where I worked with <a href="http://www.ics.uci.edu/~fowlkes/">Charless Fowlkes</a>.
	</p>
	
	<p>
		My research interests span <strong>Computer Vision</strong> and <strong>applied Machine Learning</strong> (CV/ML), 
		and their applications to autonomous vehicles and research in natural science. 
		My current research in CV/ML focuses on <a href="https://vplow.github.io"><strong>Visual Learning and Perception in the Open World</strong></a>. 
		My recent paper on this this topic was recognized for <a href="https://arxiv.org/abs/2104.02939">Best Paper / Marr Prize at ICCV 2021</a>.
		I also actively apply my algorithms to interdisciplinary research including building a high-throughput pollen analysis system,
		which was featured by the National Science Foundation as that “<a href="https://beta.nsf.gov/news/modern-computational-tools-open-new-era-fossil-pollen-research">opens a new era of fossil pollen research</a>”.
	</p>
	
	<p>
		<font color="#8B0000"><strong>
			I am looking for self-motivated PhD students starting Spring/Fall 2023 from <a href="https://engineering.tamu.edu/cse/admissions-and-aid/graduate-admissions.html">CSE department, TAMU</a>.
			PhD students will be fully funded with fellowships.
			Please feel free to contact me if interested.
		</strong></font>
	</p>


contact
<ul>
<li>gmail: aimerykong [at] gmail [dot] com</li>
<li>email: shu [at] tamu [dot] com</li>
</ul>
				</div>
    		</div>
		
<h3>Links</h3>
<ul>
	<li>
	<p><a href="http://vplow.github.io/vplow.html">The 2nd Open World Vision Workshop at CVPR'22: Visual Perception and Learning in an Open World (VPLOW)</a> </p>
	</li>
	<li>
	<p><a href="https://www.cs.umd.edu/~pulkit/DNOW_workshop/">Workshop at WACV'22: Dealing with Novelty in Open Worlds (DNOW)</a> </p>
	</li>
	<li>
	<p><a href="http://vplow.github.io/open-world-vision.html">The 1st Open World Vision Workshop at CVPR'21</a> </p>
	</li>
</ul>

		
<h3>Teaching</h3>
<ul>
	<li>
	<p>CSCE689: Visual Learning for Visual Recognition (Fall 2022). </p>
	</li>
</ul>



<h3>Research Updates</h3>
<ul>

<li>
<p>
Congratulations to <a href="https://www.linkedin.com/in/yi-ting-chen-b6984694/">Yi-Ting</a>, 
	<a href="https://www.linkedin.com/in/jinghao-shi-82371919b/">Jinghao</a>, and 
	<a href="https://www.linkedin.com/in/zelinye/">Zelin</a> on the <a href="https://eccv2022.ecva.net"  target="_blank">ECCV'22</a> 
	<img src="./aimerykong_files/ECCV2022logo.png", width="70">
	paper "<a href="https://arxiv.org/abs/2104.02904"  target="_blank">Multimodal Object Detection via Probabilistic Ensembling</a>",
	which is accepted for <font color=ff3399>oral</font> presentation!
	The proposed method was ranked the first in the <a href="https://eval.ai/web/challenges/challenge-page/1247/leaderboard/3137">KAIST challenge leaderboard</a> by the ECCV'22 submission deadline.
	Code is available in <a href="https://github.com/Jamie725/RGBT-detection">github</a>. (7/8/2022)
</a>
</li>
	
<li>
<p>
Our paper "<a href="https://ieeexplore.ieee.org/document/9799769"  target="_blank">OpenGAN: Open-Set Recognition Via Open Data Generation</a>" 
	has been published by <a href="https://ieeexplore.ieee.org/document/9799769"  target="_blank">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</a>  (6/18/2022)
</a>
</li>
		
		
<li>
<p>
Our paper "<a href="https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13917"  target="_blank">Automated identification of diverse Neotropical pollen samples using convolutional neural networks</a>" has been published by <a href="https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13917"  target="_blank">Methods in Ecology and Evolution</a>  (6/12/2022)
</a>
</li>



<li>
<p>
Our workshop <a href="http://vplow.github.io/vplow.html"  target="_blank">"Visual Perception and Learning in an Open World"</a> <img src="http://vplow.github.io/open-world-vision_files/logo.png", width="20">
will be held in conjunction with <a href="https://cvpr2022.thecvf.com/"  target="_blank">CVPR'22</a> <img src="./aimerykong_files/CVPR2022logo.png", width="70">  (4/29/2022)
</a>
</li>


<li>
<p>
Congratulations to <a href="https://www.linkedin.com/in/salshamm">Shaden</a> on the <a href="https://cvpr2022.thecvf.com/"  target="_blank">CVPR'22</a> <img src="./aimerykong_files/CVPR2022logo.png", width="70">
paper "<a href="https://arxiv.org/abs/2203.14197">Long-Tailed Recognition via Weight Balancing</a>"!
Code is available in the <a href="https://github.com/ShadeAlsha/LTR-weight-balancing">github page</a>!
(3/2/2022)
</li>




<li>
<p>
Our paper <a href="https://aimerykong.github.io/OpenGAN.html">"OpenGAN: Open-Set Recognition via Open Data Generation"</a>
received <font color=ff3399>Best Paper / Marr Prize</font> honorable mention at 
<a href="http://iccv2021.thecvf.com/" target="_blank">ICCV'21 <img src="./aimerykong_files/iccv21_logo.png", width="70"></a>.
Watch this <a href="https://youtu.be/CNYqYXyUHn0">12min video</a>
(10/12/2021) </p>
</li>


<li>
<p>
Our in-person workshop <a href="https://www.cs.umd.edu/~pulkit/DNOW_workshop/" target="_blank">Dealing with the Novelty in Open Worlds</a> 
will be held on Jan 4, 2022, in conjunction with 
<a href="http://wacv2022.thecvf.com/home" target="_blank">WACV'22 <img src="./aimerykong_files/logo_wacv2022.jpg", width="70"></a>  
(8/27/2021)
</p>
</li>


<li>
<p>
Our challenge <a href="https://eval.ai/web/challenges/challenge-page/1041/overview" target="_blank">Open-World Image Classification<img src="http://vplow.github.io/open-world-vision_files/logo.png", width="20"></a> is online now! The challenge will be held in conjunction with
our <a href="http://vplow.github.io/open-world-vision.html">Open World Vision workshop</a> and <a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR'21 <img src="./aimerykong_files/logo_cvpr2021.png", width="45"></a>
(5/14/2021)
</p>
</li>


<li>
<p>
	Congratulations to <a href="https://www.ics.uci.edu/~yunhaz5" target="_blank">Yunhan Zhao</a> on the paper <a href="https://www.ics.uci.edu/~yunhaz5/cvpr2021/cpp.html">
	"Camera Pose Matters: Improving Depth Prediction by Mitigating Pose Distribution Bias"</a>, which is accepted for <font color=ff3399>oral</font> presentation by 
<a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR'21 <img src="./aimerykong_files/logo_cvpr2021.png", width="45"></a> (03/3/2021) </p>
</li>


<li>
<p>
Our virtual workshop <a href="http://vplow.github.io/open-world-vision.html" target="_blank">Open-World Vision
	<img src="http://vplow.github.io/open-world-vision_files/logo.png", width="20"></a> will be held in conjunction with 
<a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR'21 <img src="./aimerykong_files/logo_cvpr2021.png", width="45"></a>  
(12/11/2020)
</p>
</li>



<li>
<p>
Our work is published on <a href="https://doi.org/10.1073/pnas.2007324117" target="_blank">
<img src="./aimerykong_files/pnas_logo.jpg" width="100"></a>
"<a href="https://doi.org/10.1073/pnas.2007324117" target="_blank">Improving the Taxonomy of Fossil Pollen using Convolutional Neural Networks and Superresolution Microscopy</a>",
and featured by the <a href="https://www.nsf.gov/discoveries/disc_summ.jsp?cntn_id=301568&org=NSF&from=news" target="_blank">NSF<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/NSF_logo.png/220px-NSF_logo.png", width="23"></a></a>.
(09/14/2020)
</p>
</li>

</ul>


</div>

<br>
<br>
<br>
<br>
<br>
<br>


</div>




</body>
</html>
